{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] ='OPENAI_API_KEY'\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c7780b1568c0db0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key = \"LLAMA_API_KEY\",\n",
    "    result_type = \"markdown\"\n",
    ")\n",
    "#documents = parser.load_data(\"./attention.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:12:15.093601Z",
     "start_time": "2024-09-21T12:12:15.087631Z"
    }
   },
   "id": "36f3f7feca35f32b",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='6651136a-7c88-4ab8-8afa-922b59cbd693', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Attention Is All You Need\\n\\n# Authors\\n\\nAshish Vaswani*\\n\\nNoam Shazeer*\\n\\nNiki Parmar*\\n\\nJakob Uszkoreit*\\n\\nGoogle Brain\\n\\nGoogle Brain\\n\\nGoogle Research\\n\\nGoogle Research\\n\\navaswani@google.com\\n\\nnoam@google.com\\n\\nnikip@google.com\\n\\nusz@google.com\\n\\nLlion Jones*\\n\\nAidan N. Gomez* †\\n\\nŁukasz Kaiser*\\n\\nGoogle Research\\n\\nUniversity of Toronto\\n\\nGoogle Brain\\n\\nllion@google.com\\n\\naidan@cs.toronto.edu\\n\\nlukaszkaiser@google.com\\n\\nIllia Polosukhin* ‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain.\\n\\n‡Work performed while at Google Research.\\n\\n# Conference Information\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a676f390-dd14-4399-99ab-bd594dab2841', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 1 Introduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n# 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\\n\\n# 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b83cc287-275d-4ec8-aab4-5705893a7689', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Softmax\\n\\n# Linear\\n\\n# Add & Norm\\n\\n# Feed Forward\\n\\n# Add & Norm\\n\\n# Add & Norm\\n\\n# Multi-Head Feed Forward\\n\\n# Add & Norm\\n\\n# Masked Multi-Head Attention\\n\\n# Multi-Head Attention\\n\\n# Input Embedding\\n\\n# Output Embedding\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\n# Encoder:\\n\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\n# Decoder:\\n\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='591ba10a-bd6c-4adc-8c20-7f6e61e93a44', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Scaled Dot-Product Attention\\n\\n# Multi-Head Attention\\n\\n|Linear|Concat|\\n|---|---|\\n|MatMul|SoftMax|\\n|Mask (opt)|Scaled Dot-Product Attention|\\n|Scale|Linear|\\n|MatMul|Linear|\\n| |Linear|\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\\n\\nAttention(Q, K, V) = softmax(√dkQKT)V (1)\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor √1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by √1/dk.\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional variables with mean 0 and variance 1. Then their dot product, q · k = ∑i=1dk qiki, has mean 0 and variance dk.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bba03d3d-4598-445b-8e4b-b1533ac1b36a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n# 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\\n\\n# 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f3b1532a-92ba-438e-b523-91d501da610b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\n|Layer Type|Complexity per Layer|Sequential Operations|Maximum Path Length|\\n|---|---|---|---|\\n|Self-Attention|O(n2 · d)|O(1)|O(1)|\\n|Recurrent|O(k · n · d)|O(n)|O(logk(n))|\\n|Convolutional|O(r · n · d)|O(1)|O(n/r)|\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos, 2i) = sin(pos/100002i/dmodel)\\n\\nPE(pos, 2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='43c31740-b98b-4dd8-85dd-9e8117109c27', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Training\\n\\nThis section describes the training regime for our models.\\n\\n# 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n# 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n# 5.3 Optimizer\\n\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = dmodel−0.5· min(step_num−0.5, step_num · warmup_steps−1.5)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n# 5.4 Regularization\\n\\nWe employ three types of regularization during training:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e6ce89d6-b361-47ac-95ff-cd364829bdbe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n|Model|BLEU EN-DE|BLEU EN-FR|Training Cost (FLOPs) EN-DE|Training Cost (FLOPs) EN-FR|\\n|---|---|---|---|---|\\n|ByteNet [18]|23.75| | | |\\n|Deep-Att + PosUnk [39]| |39.2| |1.0 · 1020|\\n|GNMT + RL [38]|24.6|39.92|2.3 · 1018|1.4 · 1020|\\n|ConvS2S [9]|25.16|40.46|9.6 · 1019|1.5 · 1020|\\n|MoE [32]|26.03|40.56|2.0 · 10|1.2 · 1020|\\n|Deep-Att + PosUnk Ensemble [39]| |40.4| |8.0 · 1021|\\n|GNMT + RL Ensemble [38]|26.30|41.16|1.8 · 1019|1.1 · 1021|\\n|ConvS2S Ensemble [9]|26.36|41.29|7.7 · 10|1.2 · 10|\\n|Transformer (base model)|27.3|38.1| |3.3 · 1018|\\n|Transformer (big)|28.4|41.8| |2.3 · 10|\\n\\n# Residual Dropout\\n\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\n# Label Smoothing\\n\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5.\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2a9cbba0-ab42-454a-b6f8-f68d1d9d9b62', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Table 3: Variations on the Transformer architecture\\n\\nUnlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|N|dmodel|dff|h|dk|dv|Pdrop|ϵls|train steps|PPL (dev)|BLEU (dev)|params×106| |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|base|6|512|2048|8|64|64|0.1|0.1|100K|4.92|25.8|65|\\n|(A)|1|512|512| | | | | |5.29|24.9| | |\\n| |4|128|128| | | | | |5.00|25.5| | |\\n| |16|32|32| | | | | |4.91|25.8| | |\\n| |32|16|16| | | | | |5.01|25.4| | |\\n|(B)| |16| | | | | | |5.16|25.1|58| |\\n| | |32| | | | | | |5.01|25.4|60| |\\n| |2| | | | | | | |6.11|23.7|36| |\\n| |4| | | | | | | |5.19|25.3|50| |\\n| |8| | | | | | | |4.88|25.5|80| |\\n|(C)|256| |32|32| | | | |5.75|24.5|28| |\\n| |1024| |128|128| | | | |4.66|26.0|168| |\\n| |1024| | | | | | | |5.12|25.4|53| |\\n| |4096| | | | | | | |4.75|26.2|90| |\\n| | | | | | |0.0| | |5.77|24.6| | |\\n|(D)| | | | | |0.2| | |4.95|25.5| | |\\n| | | | | | |0.0| | |4.67|25.3| | |\\n| | | | | | |0.2| | |5.47|25.7| | |\\n|(E)| | | | | |positional embedding instead of sinusoids| | |4.92|25.7| | |\\n|big|6|1024|4096|16| |0.3| |300K|4.33|26.4|213| |\\n\\nWe used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1d82323d-1d8d-4f32-9552-cf041e11ca31', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|Parser|Training|WSJ 23 F1|\\n|---|---|---|\\n|Vinyals & Kaiser el al. (2014) [37]|WSJ only, discriminative|88.3|\\n|Petrov et al. (2006) [29]|WSJ only, discriminative|90.4|\\n|Zhu et al. (2013) [40]|WSJ only, discriminative|90.4|\\n|Dyer et al. (2016) [8]|WSJ only, discriminative|91.7|\\n|Transformer (4 layers)|WSJ only, discriminative|91.3|\\n|Zhu et al. (2013) [40]|semi-supervised|91.3|\\n|Huang & Harper (2009) [14]|semi-supervised|91.3|\\n|McClosky et al. (2006) [26]|semi-supervised|92.1|\\n|Vinyals & Kaiser el al. (2014) [37]|semi-supervised|92.1|\\n|Transformer (4 layers)|semi-supervised|92.7|\\n|Luong et al. (2015) [23]|multi-task|93.0|\\n|Dyer et al. (2016) [8]|generative|93.3|\\n\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\n# Acknowledgements\\n\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n1. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n2. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n3. Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n4. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ae574851-0f75-464c-80d4-eaf6c66e326e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n2. Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n3. Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n4. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n5. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n6. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n7. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n8. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n9. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n10. Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n11. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n12. Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n13. Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n14. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n15. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n16. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n17. Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n18. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n19. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n20. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='63795f7c-6a13-482d-84f5-e0e4a0a60fc5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# References\\n\\n1. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\\n2. David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n3. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n4. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n5. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n6. Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n7. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n8. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n9. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n10. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n11. Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n12. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n13. Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n14. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n15. Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n16. Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='757fff6a-4d4b-4450-8d70-abbe25b7cfad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Input-Input Layer 5\\n\\n# Attention Visualizations\\n\\nIt is this spirit that a majority of American governments have passed new laws since 2009.\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.\\n\\nAttentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b643f362-59e0-4ac3-b230-8ba7701cbbef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Input-Input Layer 5\\n\\nThe law will never be perfect, but its application should be just.\\n\\nThis is what we are missing, in my opinion.\\n\\n# Figure 4\\n\\nTwo attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='19c7c1a7-7f61-4abf-9be7-ee2780aad6af', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Input-Input Layer 5\\n\\nThe law will never be perfect, but its application should be just.\\n\\n# What we are missing\\n\\nIn my opinion...\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "<class 'llama_index.core.schema.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(documents)\n",
    "print(type(documents[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:05:33.793874Z",
     "start_time": "2024-09-21T12:05:33.778526Z"
    }
   },
   "id": "d2868b7a80edf678",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='84aa7523-2d89-4cdb-aa3c-bfe51cc403c6', embedding=None, metadata={'page_label': '1', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='B.Tech. Computer Science and Engineering (Artificial Intelligence & Machine \\nLearning)  \\nRevised Scheme and Curriculum  \\nI year Courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 1)  \\n1.  CSE       110 Problem Solving - C Programming  3 \\n2.  CSE       120 Python Programming  3 \\n3.  CSE       130 IOT - 1  3 \\n4.  ENG        126 English I  3 \\nQuarter -II (Year 1)  \\n5.  CSE       160 Data Structures  3 \\n6.  MTH        141 Probability & Statistics  3 \\n7.  CSE       170 Java Basics with OOPs Concepts  3 \\n8.  CSE       140 Database Management Systems  3 \\nQuarter -III (Year 1)  \\n9.  MTH        151 Calculus  3 \\n10.  CSE       180 Advanced Python Programming  3 \\n11.  CSE       150 Web Design – Frontend Development  3 \\n12.  PHY       145 Physics  3 \\nQuarter -IV (Year 1)  \\n13.  INT       200 Internship 1   6 \\nTOTAL  42 \\n \\nII Year Courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 2)  \\n14. CSE       210  IOT - 2 3 \\n15. CSE       220  Web Programming & Scripting  3 \\n16. MTH       271  Linear Algebra  3 \\n17. CSE       250  Operating Systems  3 \\nQuarter -II (Year 2)  \\n18. CSE       270  Design and Analysis of Algorithms  3 \\n19. CSE       290  Computer Networks  3 \\n20. CSE       280  Advanced  Java  3 \\n21. CSE       240  Data Science with R  3 \\nQuarter -III (Year 2)  \\n22. INT       300  Internship 2  6 \\n \\nQuarter -IV (Year 2)  \\n23. CSE       230  Mobile Programming  3 \\n24. MTH       221  Differential Equations  3 \\n25. CSE       260  Data Visualization Tools and Techniques  3 \\n26. ENG       226  English -2 3 \\nTOTAL  42 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='39b9be6f-bd2e-431e-8b89-b97a13bf7b53', embedding=None, metadata={'page_label': '2', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='III Year courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 3)  \\n27. INT       400  Internship 3  6 \\nQuarter -II (Year 3)  \\n28. MTH       351  Advanced Statistics  3 \\n29. CSE       330  Cloud Computing  3 \\n30. CSE       310  Artificial Intelligence and Machine Learning  3 \\n31. CSE       320 Data Mining  3 \\nQuarter -III (Year 3)  \\n32. CSE       340  Machine Learning 1  3 \\n33. CSE       370  Cyber Security  3 \\n34. MTH       331  Discrete Mathematics  3 \\n35. CSE       360  Deep Learning  3 \\nQuarter -IV (Year 3)  \\n36. CSE       345  Elective I – Block Chain Technologies  3 \\n37. CSE       380  Natural Language Processing  3 \\n38. CHE       305  Chemistry  3 \\n39. CSE       355  Elective II - Data Science Professional \\nCertification      3 \\nTOTAL  42 \\n \\n \\nIV Year Courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 4)  \\n40. CSE       440  Reinforcement Learning  3 \\n41.  Elective III  3 \\n42. CSE       410  Computer Vision  3 \\n43.  Elective IV  3 \\nQuarter -II (Year 4)  \\n44. CSE       420  Software Engineering & Testing  3 \\n45.  Elective V  3 \\n46.  Elective  VI 3 \\n47. HUM       406  Humanities Course  3 \\nQuarter -III & IV  (Year 4)  \\n48. INT       500  Internship 4  6 \\n49.  INT       510  Internship 5  6 \\nTOTAL  36 \\n \\n \\nTOTAL  CREDITS : 162  \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='54eac9bf-1adc-4c20-9c42-230c21283f37', embedding=None, metadata={'page_label': '3', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='B.Tech. Computer Science and Engineering (Cybersecurity &  Internet of Things)  \\nRevised Scheme and Curriculum  \\n \\nI year Courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 1)  \\n1.  CSE       110 Problem Solving - C Programming  3 \\n2.  CSE       120 Python Programming  3 \\n3.  CSE       130 IOT - 1  3 \\n4.  ENG        126 English I  3 \\nQuarter -II (Year 1)  \\n5.  CSE       160 Data Structures  3 \\n6.  MTH        141 Probability & Statistics  3 \\n7.  CSE       170 Java Basics with OOPs Concepts  3 \\n8.  CSE       140 Database Management Systems  3 \\nQuarter -III (Year 1)  \\n9.  MTH        151 Calculus  3 \\n10.  CSE       180 Advanced Python Programming  3 \\n11.  CSE       150 Web Design – Frontend Development  3 \\n12.  PHY       145 Physics  3 \\nQuarter -IV (Year 1)  \\n13.  INT       200 Internship 1   6 \\nTOTAL  42 \\n \\nII Year Courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 2)  \\n14. CSE       210  IOT - 2 3 \\n15. CSE       220  Web Programming & Scripting  3 \\n16. MTH       271  Linear Algebra  3 \\n17. CSE       250  Operating Systems  3 \\nQuarter -II (Year 2)  \\n18. CSE       270  Design and Analysis of Algorithms  3 \\n19. CSE       290  Computer Networks  3 \\n20. CSE       280  Advanced  Java  3 \\n21. CSE       240  Data Science with R  3 \\nQuarter -III (Year 2)  \\n22. INT       300  Internship 2  6 \\nQuarter -IV (Year 2)  \\n23. CSE       230  Mobile Programming  3 \\n24. MTH       221  Differential Equations  3 \\n25. CSE       260  Data Visualization Tools and Techniques  3 \\n26. ENG       226  English -2 3 \\nTOTAL  42 \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='06852082-fb02-40bc-90f4-202830b8de04', embedding=None, metadata={'page_label': '4', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='III Year courses  \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 3)  \\n27. INT       400  Internship 3  6 \\nQuarter -II (Year 3)  \\n28. MTH       351  Advanced Statistics  3 \\n29. CSE       330  Cloud Computing  3 \\n30. CSE       310  Artificial Intelligence and Machine Learning  3 \\n31. CSE       320  Data Mining  3 \\nQuarter -III (Year 3)  \\n32. CSE       350 IOT Sensors, Microcontrollers & Signal \\nProcessing  3 \\n33. CSE       370 Cyber Security  3 \\n34. CSE       390 Wireless Networks  3 \\n35. MTH       331  Discrete Mathematics  3 \\nQuarter -IV (Year 3)  \\n36. CSE       345  Elective I – Block Chain Technologies  3 \\n37. CSE       372  Advanced Cyber Security  3 \\n38. CHE        305 Chemistry  3 \\n39. CSE       355  Elective II - Data Science Professional \\nCertification      3 \\nTOTAL  42 \\n \\n \\nIV Year Courses  \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 4)  \\n40. ELE Elective III  3 \\n41. CSE       430  Design Thinking  3 \\n42. CSE       420  Software Engineering & Testing  3 \\n43. CSE       450  IoT Security  3 \\nQuarter -II (Year 4)  \\n44. ELE Elective IV  3 \\n45. ELE Elective V  3 \\n46. ELE Elective VI  3 \\n47. HUM       406  Humanities Course  3 \\nQuarter -III & IV (Year 4)  \\n48. INT       500  Internship 4  6 \\n49.  INT       510  Internship 5  6 \\nTOTAL  36 \\n \\n \\nTOTAL CREDITS: 162   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a64cf7be-5c98-4aaa-b114-fed059d3487c', embedding=None, metadata={'page_label': '5', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='B.Tech. Computer Science and Engineering (Artificial Intelligence & Data Analytics)  \\nRevised Scheme and Curriculum  \\n \\nI year Courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 1)  \\n1.  CSE       110 Problem Solving - C Programming  3 \\n2.  CSE       120 Python Programming  3 \\n3.  MTH        141  Probability & Statistics  3 \\n4.  ENG        126 English I  3 \\nQuarter -II (Year 1)  \\n5.  CSE       160 Data Structures  3 \\n6.  CSE       130 IOT - 1 3 \\n7.  CSE       170 Java Basics with OOPs Concepts  3 \\n8.  PHY       145  Physics  3 \\nQuarter -III (Year 1)  \\n9.  MTH        151 Calculus  3 \\n10.  CSE       180 Advanced Python Programming  3 \\n11.  CSE       150 Web Design – Frontend Development  3 \\n12.  CSE       140 Database Management Systems  3 \\nQuarter -IV (Year 1)  \\n13.  INT       200 Internship 1   6 \\nTOTAL  42 \\n \\nII Year Courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 2)  \\n14. CSE       210  IOT - 2 3 \\n15. CSE       220  Web Programming & Scripting  3 \\n16. MTH       271  Linear Algebra  3 \\n17. CSE       250  Operating Systems  3 \\nQuarter -II (Year 2)  \\n18. CSE       270  Design and Analysis of Algorithms  3 \\n19. CSE       290  Computer Networks  3 \\n20. CSE       280 Advanced  Java  3 \\n21. CSE       240  Data Science with R  3 \\nQuarter -III (Year 2)  \\n22. INT       300  Internship 2  6 \\nQuarter -IV (Year 2)  \\n23. CSE       230  Mobile Programming  3 \\n24. MTH       221  Differential Equations  3 \\n25. CSE       260  Data Visualization Tools and Techniques  3 \\n26. ENG       226  English -2 3 \\nTOTAL  42 \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6c8f907a-570a-4bf7-a36c-611e6cd6b6c0', embedding=None, metadata={'page_label': '6', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='III Year courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 3)  \\n27. INT       400  Internship 3  6 \\n  Quarter -II (Year 3)   \\n28. MTH       351  Advanced Statistics  3 \\n29. CSE       330  Cloud Computing  3 \\n30. CSE       310  Artificial Intelligence and Machine Learning  3 \\n31. CSE       325  Big Data Analytics  3 \\nQuarter -III (Year 3)  \\n32. CSE       340  Machine Learning 1  3 \\n33. CHE       305  Chemistry  3 \\n34. MTH       331  Discrete Mathematics  3 \\n35. CSE       360  Deep Learning  3 \\nQuarter -IV (Year 3)  \\n36. CSE       345  Elective I – Block Chain Technologies  3 \\n37. CSE       380  Natural Language Processing  3 \\n38. CSE       370  Cyber Security  3 \\n39. CSE       355  Elective II - Data Science Professional \\nCertification      3 \\nTOTAL  42 \\n \\n \\nIV Year Courses  \\n \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 4)  \\n40. CSE       440  Reinforcement Learning  3 \\n41. ELE Elective III  3 \\n42. CSE       410  Computer Vision  3 \\n43. ELE Elective IV  3 \\nQuarter -II (Year 4)  \\n44. CSE       420  Software Engineering & Testing  3 \\n45. ELE Elective V  3 \\n46. ELE Elective VI  3 \\n47. HUM       406  Humanities Course  3 \\nQuarter -III & IV  (Year 4)  \\n48. INT       500  Internship 4  6 \\n49.  INT       510  Internship 5  6 \\nTOTAL  36 \\n \\n \\n \\nTOTAL CREDITS: 162  \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3e053c06-09ef-4aba-ab1c-18db51a9d896', embedding=None, metadata={'page_label': '7', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='B.Tech. Computer Science and Medical Engineering  \\nRevised Scheme and Curriculum  \\n \\nI year Courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 1)  \\n1.  CSE       110 Problem Solving - C Programming  3 \\n2.  CSE       120 Python Programming  3 \\n3.  MTH        141  Probability & Statistics   3 \\n4.  ENG        126 English I  3 \\nQuarter -II (Year 1)  \\n5.  CSE       160 Data Structures  3 \\n6.  CSE       130 IOT - 1 3 \\n7.  CSE       170 Java Basics with OOPs Concepts  3 \\n8.  PHY       145  Physics  3 \\nQuarter -III (Year 1)  \\n9.  MTH        151 Calculus  3 \\n10.  CSE       180 Advanced Python Programming  3 \\n11.  CSE       150 Web Design – Frontend Development  3 \\n12.  CSE       140 Database Management Systems  3 \\nQuarter -IV (Year 1)  \\n13.  INT       200 Internship 1   6 \\nTOTAL  42 \\n \\nII Year Courses  \\nCourse \\nNumber  Course code  Course Title  Credits  \\nQuarter -I (Year 2)  \\n14. CSE       210  IOT - 2 3 \\n15. MED       215  Introduction to Medical Science for Engineers  3 \\n16. MTH       221  Differential Equations  3 \\n17. CSE       220  Web Programming & Scripting  3 \\nQuarter -II (Year 2)  \\n18. MED       235  Introduction to Medical Devices and Systems \\nfor Engineers  3 \\n19. CSE       240  Data Science with R  3 \\n20. CSE       250  Operating Systems  3 \\n21. CSE       260 Data Visualization Tools and Techniques  3 \\nQuarter -III (Year 2)  \\n22. INT       300  Internship 2  6 \\nQuarter -IV (Year 2)  \\n23. CHE       305  Chemistry  3 \\n24. CSE       270  Design and Analysis of Algorithms  3 \\n25. CSE       290  Computer Networks  3 \\n26. ENG       226  English -2 3 \\nTOTAL  42 \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9f6c044c-2000-4dc6-b84f-74803548a94e', embedding=None, metadata={'page_label': '8', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='III Year courses  \\n \\nCourse Number  Course code  Course Title  Credits  \\nQuarter -I (Year 3)  \\n27. INT       400  Internship 3  6 \\nQuarter -II (Year 3)  \\n28. CSE       310  Artificial Intelligence and Machine Learning  3 \\n29. MED      315 Introduction to Medical Procedures  3 \\n30.  Elective 1  3 \\n31. MED     335  Medical Engineering 1  3 \\nQuarter -III (Year 3)  \\n32. MED       345  Medical Engineering 2  3 \\n33. MED      355  Medical Engineering 3  3 \\n34. MTH       351  Advanced Statistics  3 \\n35. MED       365  Medical Engineering 4  3 \\nQuarter -IV (Year 3)  \\n36. CSE       340  Machine  Learning 1  3 \\n37. CSE       360  Deep Learning  3 \\n38. MTH       271  Linear Algebra  3 \\n39.  Elective 2  3 \\nTOTAL  42 \\n \\nIV Year Courses  \\n \\nCourse \\nNumber  Course code  Course Title Credits  \\nQuarter -I (Year 4)  \\n40. CSE       410  Computer Vision  3 \\n41. CSE       420  Software Engineering & Testing  3 \\n42. ELE Elective 3  3 \\n43. ELE Elective 4  3 \\nQuarter -II (Year 4)  \\n44. ELE Elective 5  3 \\n45. ELE Elective 6  3 \\n46. CSE       440  Reinforcement Learning  3 \\n47. HUM       406  Humanities  3 \\nQuarter -III & IV (Year 4)  \\n48. INT       500  Internship 4  6 \\n49.  INT       510  Internship 5  6 \\nTOTAL  36 \\n \\nTOTAL CREDITS: 162', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='889ca264-47a8-47cf-ba0e-60d3e786a74d', embedding=None, metadata={'page_label': '9', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. CSE 110  Problem Solving - C Programming   3 Cr \\n \\nThis is the first  course in Problem solving with C Programming. Topics include \\nunderstanding and applying programming logic and logic development, constants, variables, data \\ntypes, decision making, arrays, pointers and file processing  \\n \\nCO1:  Understand the problem solving a spect and write algorithm.  \\nCO2:  Understand and apply various concepts of C programming.  \\nCO3:  Ability to work with arrays and user defined datatypes  \\nCO4:  Develop and implement applications in C using functions and pointers  \\nCO5:  Design applications using  file processing.  \\n \\n \\nModule 1  \\n\\uf0b7 Logical Thinking & Programming Logic  \\n\\uf0b7 Solving Logical Puzzle  \\n\\uf0b7 Understanding a Program and its logic  \\n\\uf0b7 Understanding a Problem and Logic development  \\n\\uf0b7 Algorithm & Flow Chart  \\nModule 2  \\n\\uf0b7 Introduction to Programming  \\n\\uf0b7 Constants, Variables &  Data Types  \\n\\uf0b7 Operators & Expressions  \\n\\uf0b7 Managing Input & output operations  \\n\\uf0b7 Decision Making – Branching & Looping  \\nModule 3  \\n\\uf0b7 Storage Classes  \\n\\uf0b7 Arrays  \\n\\uf0b7 Structure and Union  \\nModule 4  \\n\\uf0b7 Pointers  \\n\\uf0b7 Dynamic Memory Allocation   \\n\\uf0b7 Link list  \\nModule 5  \\n\\uf0b7 File Processing  \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='851201f8-9a14-4dd8-9723-5d022ae35d53', embedding=None, metadata={'page_label': '10', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. CSE  120 Python Programming   3 Cr \\n \\nAn introductory course in Python Programming. The course is approached as the foundation \\nfor the courses to follow.  Topics include Basic Syntax, Variable and data types, Operator, conditional \\nand control statements, string manipulations, functions, modules, attributes, match and search \\nfunctions and applications.  \\n \\nCO1:  Explore various operators and Implement Conditionals and Loops for Python Programs.  \\nCO2:  Represent compound data using Python lists, tuples, dictionaries and  string.  \\nCO3:  Implement and analyze the concept of modular programming and error handling in python.  \\nCO4:  Design the program models using Object -Oriented Programming concepts such as encapsulation, \\ninheritance and polymorphism as used in Python.  \\nCO5:  Identify and implement the various string manipulate operations using regular expressions.  \\n \\nModule 1  \\n\\uf0b7 Basic Syntax, Variable and data types, Operator  \\n\\uf0b7 Conditional Statements  \\n\\uf0b7 Looping  \\n\\uf0b7 Control Statements  \\nModule 2  \\n\\uf0b7 String Manipulation  \\n\\uf0b7 Lists  \\n\\uf0b7 Tuple  \\n\\uf0b7 Dictionaries  \\nModule 3  \\n\\uf0b7 Functions  \\n\\uf0b7 Modules  \\n\\uf0b7 Input/output  \\n\\uf0b7 Exception Handling  \\nModule 4:  \\n\\uf0b7 Class and object   \\n\\uf0b7 Attributes  \\n\\uf0b7 Inheritance   \\n\\uf0b7 Overloading  \\n\\uf0b7 Overriding   \\n\\uf0b7 Data hiding   \\nModule 5  \\n\\uf0b7 Regular expressions  \\n\\uf0b7 Match function   \\n\\uf0b7 Search function   \\n\\uf0b7 Matching VS Searching  \\n\\uf0b7 Modifiers  \\n\\uf0b7 Patterns  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a0696ccf-bd30-4206-b0ff-b000dc8104a5', embedding=None, metadata={'page_label': '11', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. CSE 130  IoT - 1  3 Cr  \\n \\nThis is the first course in Internet of Things. It starts with an introduction to basic electronics \\nand electronic components, integrated circuits and boards, Arduino micro controllers and the \\napplication of microcontrollers to build  internet of things.  \\n \\nCO1:  Understand the basics of Electrical circuits, loads and breadboard connection  \\nCO2:  Model a circuit and analyze its operation based on the concepts learned from various types of \\nmaterial, circuit elements & electronic devices  \\nCO3:  Ability to understand and develop simple projects using the concept of Integrated circuits, logic \\ngates and ARDUINO board architecture  \\nCO4:  Ability to develop project by connecting and controlling various electronic devices by using \\nArduino platform and programming concepts  \\nCO5:  Ability to build an IoT project by understanding sensors and its automated applications, motor \\ncontrol and communication technologies  \\n \\n \\nModule 1  \\n\\uf0b7 Introduction to Electrical Sources  \\n\\uf0b7 Battery, types and its ratings  \\n\\uf0b7 Switch and Types  \\n\\uf0b7 Introduction to Electrical and Electronic loads  \\n\\uf0b7 Low power loads: LED, Buzzer, Speaker DC Motor, Gear Motor, Servomotor, Display  \\n\\uf0b7 Breadboard basics  \\n \\nModule 2  \\n\\uf0b7 Introduction to conductors, Insulators, semiconductors  \\n\\uf0b7 Circuit elements: Resistors, Inductors, Capacitors  \\n\\uf0b7 Devices and its operation: Diode, LED, Photodiode, LDR, Transistor: BJT, PNP, NPN, Photo \\ntransistor, IR Receiver, Thermistor  \\n \\nModule 3  \\n\\uf0b7 Integrated Circuits Pin diagram and applications: IC741 -  555 Timer -  LM358, LM386, \\nLM353 - Counter IC  \\n\\uf0b7 Logic gates: AND, OR, NAND, NOR, NOT, EX -OR, EX -NOR  \\n\\uf0b7 Introduction to IoT and its applications  \\n\\uf0b7 ARDUINO: Board architecture  \\n \\nModule 4  \\n\\uf0b7 Programming with ARDUINO UNO board  \\n\\uf0b7 Working with LEDs  \\n\\uf0b7 Operating LED with time delay  \\n\\uf0b7 Working with digital  switch  \\n\\uf0b7 Adjusting voltage using potentiometer  \\n\\uf0b7 Introduction to sensor Integration with ARDUINO  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='31630180-3ffc-4dcc-818c-918756430e87', embedding=None, metadata={'page_label': '12', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Module 5  \\n\\uf0b7 Sensor and its automated applications using ARDUINO: LDR:  illumination control, \\nUltrasonic sensor, DHT 11 sensor, PIR Sensor  \\n\\uf0b7 Working with Servo motor  \\n\\uf0b7 Establishment of communication using Bluetooth  \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a8453e27-15df-467b-8802-d11ed8248ef4', embedding=None, metadata={'page_label': '13', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. ENG  126  English -1  3 Cr  \\n \\nAs the first course in English, it focuses on reading, vocabulary building, grammar, \\ncomprehension, speaking, writing, and communicating.  Confidence building and technical \\ncommunication are considered the essential outcomes of the course.  \\n \\nCO1:  Understanding of the Process of Communication  \\nCO2:  Improved Listening comprehension of English language and ability to clear IELTS Listening \\nmodule  \\nCO3:  Improved Speaking skills of English language and ability to clear IELTS Speaking module  \\nCO4:  Improved Reading comprehension of English language and ability to clear IELTS Reading \\nmodule  \\nCO5:  Improved Writing skills of English language and ability to clear IELTS Writing module  \\n \\n \\nModule 1 – COMMUNICATIVE ENGLISH: AN INTRODUCTION  \\nCommunication: definition, types - Process of Communication - Channels of Communication - Barriers \\nof Communication - Paralinguistic features - Communicative English: Definition, Purpose – LSRW - \\nGeneral Awarenes s on Language Proficiency Tests: IELTS, TOEFL, PTE  \\n \\nModule 2 - LISTENING  \\nActive Listening and Passive Listening – Barriers to Listening – Listening Exercises: conversations \\nbetween two people set in an everyday social context, monologues set in an everyday social context, , \\nconversations between up to four people set in an educational or training context,  monologues on an \\nacademic subject etc.  \\n \\nModule 3 – SPEAKING  \\nBasic Phonetics - Pronunciation - Fluency and Coherence - Syllables -Public Speaking -Attending a n \\nInterview,  Casual Conversations, Group Discussion.  \\n \\nModule 4 – reading  \\nEffective reading -Types of Reading – Reading Comprehension:  multiple choice, identifying \\ninformation, identifying the writer’s views/claims, matching information, matching headings , \\nmatching features, matching sentence endings, sentence completion, summary completion, note \\ncompletion, table completion, flow -chart completion, diagram label completion and short -answer \\nquestions  \\n \\nModule 5 – WRITING  \\n Formal Letter: Introduction, Types, Format and Structure – Reports: Academic and Business – Short \\nStory Writing: Eight -point Arc of Short Story Writing, Writing dialogues for short stories, Story \\nWriting based on Pictures, Story Writing using an Outline –Resume and Cover Letter:  Difference \\nbetween CV, Resume and Biodata, Resume Writing, Cover Letter Writing – Writing a Movie Review: \\nStructure and Vocabulary – Explaining Flow Chart/Bar Diagram: Structure and Vocabulary – Essay \\nWriting  \\n \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='39ef4a76-c324-4395-b9ec-d4832f891021', embedding=None, metadata={'page_label': '14', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5. CSE 160  Data Structures   3 Cr \\n \\nThis is the first course in data structures. It deals with arrays, pointers, use of arrays, \\nsearching, sorting  and analysis of algorithms,  \\n \\nCO1  To impart a thorough understanding and application of dynamic data structure.   \\nCO2  To design and implement the linear data structures like stack and queue using dynamic \\nimplementation.   CO3  To represent and manipulate data using nonlinear data structures like trees and graphs to design \\nalgorithms for various applications.   \\nCO4  To understand and apply elementary algorithms: sorting, searching and hashing.         \\nCO5  To analyze the efficiency of programs based on time and space complexity.                 \\n \\n \\nModule 1  \\n\\uf0b7 Arrays & Vectors  \\n\\uf0b7 Static and Dynamic Arrays  \\n\\uf0b7 Linked List, Circular List  \\n\\uf0b7 Pointers  \\n\\uf0b7 Implentation of list using pointers  \\nModule 2  \\n\\uf0b7 Stack  \\n\\uf0b7 Queue  \\n\\uf0b7 Implentation using Arrays and Linked List  \\nModule 3  \\n\\uf0b7 Tree \\n\\uf0b7 Binary Tree  \\n\\uf0b7 BST \\n\\uf0b7 Graph  \\n\\uf0b7 Traversal algorithm  \\nModule 4  \\n\\uf0b7 Searching Algorithms  \\n\\uf0b7 Sorting Algorithms  \\n\\uf0b7 Hashing  \\nModule 5  \\n\\uf0b7 Analysis of Algorithm  \\n\\uf0b7 Efficiency Calculation  \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5a91fb69-f845-46c7-a7d5-3ac37e44f09c', embedding=None, metadata={'page_label': '15', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6. MTH 141  Probability and Statistics   3 Cr  \\n \\nThis is the first course in Probability and Statistics. Topics include Variable and its Types, \\nSampling, Experimental Design, Histograms, Clusters, Time Series Graph, Randomness, Simulation, \\nAnaova, T -Test, and Distributions  \\n \\nCO1:  To learn the basics of st atistics and sampling techniques  \\nCO2:  To study about frequency distributions and related graphs  \\nCO3:  To acquire the knowledge of measures of central tendency and variation  \\nCO4:  To obtain the basic ideas of probability theory through random experiments  \\nCO5:  To learn the beginner level concepts of inferential statistics  \\n \\nModule 1  \\n\\uf0b7 Introduction to statistics  \\n\\uf0b7 Individuals and variables  \\n\\uf0b7 Population  \\n\\uf0b7 Parameter  \\n\\uf0b7 Types of variables: Quantitative, Categorical  \\n\\uf0b7 Sampling: Random, Stratified, Systematic, Cluster and Errors  \\n\\uf0b7 Experimental design  \\n\\uf0b7 Avoiding bias in survey design  \\n\\uf0b7 Randomization: Placebo effect, blocked randomization, Blinding  \\nModule 2  \\n\\uf0b7 Frequency histograms  \\n\\uf0b7 Frequency table  \\n\\uf0b7 Relative Frequency table  \\n\\uf0b7 Stem and leaf plot  \\n\\uf0b7 Distribution  \\n\\uf0b7 Clusters  \\n\\uf0b7 Peaks  \\n\\uf0b7 gaps and Outliers  \\n\\uf0b7 Time series graph, bar graph, Pareto chart, and pie chart  \\nModule 3  \\n\\uf0b7 Measures of central tendency: Mode, Median, Mean, Trimmed mean  \\n\\uf0b7 Weighted average, Central tendencies and distributions  \\n\\uf0b7 Measures of var iation: Standard deviation, Variance  \\n\\uf0b7 Coefficient of variation  \\n\\uf0b7 Chebyshev’s Theorem  \\n\\uf0b7 Percentiles, IQR, Box and Whisker plots  \\n\\uf0b7 Scatter diagrams  \\nModule 4  \\n\\uf0b7 Basic theoretical probability  \\n\\uf0b7 Probability using sample spaces  \\n\\uf0b7 Probability of basic set operations  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e2aa69f0-d214-4f08-9194-c3b79101f649', embedding=None, metadata={'page_label': '16', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Experiment al probability, randomness, probability and simulation  \\n\\uf0b7 Addition and multiplication rule for probability, conditional probability and independence  \\nModule 5  \\n\\uf0b7 Linear correlation, Linear regression, Least square criterion  \\n\\uf0b7 Coefficient of determination  \\n\\uf0b7 T-test \\n\\uf0b7 Anova \\n\\uf0b7 Normal distributions and empirical rule  \\n\\uf0b7 Z-scores and Probabilities, Using Z -tables  \\n\\uf0b7 Sampling distributions and the central limit theorem  \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='242e4a37-052f-455b-9a4f-924ebec24100', embedding=None, metadata={'page_label': '17', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7. CSE 170  Java Basics with OOPs  Concepts   3 Cr  \\n \\nJava Programming is taught as an essential building block of Computer science and \\nengineering program. It presents the basics of object oriented programming, and its applications.  \\n \\nCO1:  Design and implement basic data types, array and control flow  using J2SE  \\nCO2:  Demonstrate constructors and method in java  \\nCO3:  Implement Java Program using object -oriented concepts Data Abstraction, Polymorphism  \\nCO4  Apply the concepts of java packages, inheritance, and Exception handling mechanisms for real -\\nworld problems.  \\nCO5  Demonstrate multitasking using Threads and Develop simple applications using GUIs and event -\\ndriven  \\nProgramming  \\n \\n \\nModule 1  \\n\\uf0b7 Object Oriented Programming Basics  \\n\\uf0b7 Class and Objects  \\n\\uf0b7 Variables and data types  \\n\\uf0b7 Conditional and looping constructs  \\n\\uf0b7 Arrays  \\nModule 2  \\n\\uf0b7 Fields and Methods  \\n\\uf0b7 Constructors  \\n\\uf0b7 Overloading methods  \\n\\uf0b7 Garbage collection  \\n\\uf0b7 Nested classes  \\nModule 3  \\n\\uf0b7 Inheritance  \\n\\uf0b7 Overriding methods  \\n\\uf0b7 Polymorphism  \\n\\uf0b7 Making methods and classes final  \\nModule 4  \\n\\uf0b7 Abstract classes and methods  \\n\\uf0b7 Interfaces  \\n\\uf0b7 String and String Conversions  \\n\\uf0b7 Exception Handling  \\nModule 5  \\n\\uf0b7 Packages  \\n\\uf0b7 Applets  \\n\\uf0b7 Threads  \\n\\uf0b7 AWT & Swings Basics  \\n\\uf0b7 Database Connectivity  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='eabddf34-8d8a-4aa0-bad7-92cda056b251', embedding=None, metadata={'page_label': '18', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8. CSE 140          Database Management Systems   3 Cr  \\n \\n As the first course in database management system, it  considers database types, naming \\nconventions, look up tables, updating  and deleting data, database design and database  schema.  \\n \\nCO1:  Explore the various methods to develop relational databases and their  types  \\nCO2:  Design and implement ER diagram & database schema for a given problem and apply \\nnormalization for the given database application.  \\nCO3:  Build database using Data Definition Language Statements and perform basic CRUD \\noperations using Data Manipulation Language statements like Insert, Update and Delete.  \\nCO4:  To know about joins and types of joins, Also learn how to join the tables and about \\nSubqueries, types of operators like union.  \\nCO5:  To expose students to design and develop a database for real -time applications.  \\n  \\n \\nModule 1  \\n\\uf0b7 Relational databases and Codd rules  \\n\\uf0b7 Tools You Need f or Database Design  \\n\\uf0b7 MySQL Workbench  \\n\\uf0b7 Database and Types  \\n \\nModule 2  \\n\\uf0b7 Identifying Entities  \\n\\uf0b7 Defining the Attributes  \\n\\uf0b7 Normalization  \\n\\uf0b7 Data Types and Precision  \\n\\uf0b7 Integrity Constraints  \\n\\uf0b7 Naming Conventions  \\n\\uf0b7 Lookup Tables and Auditing  \\n \\nModule 3  \\n\\uf0b7 Command Line & Inserting Data  \\n\\uf0b7 Basic Select Queries  \\n\\uf0b7 Updating & Deleting Data  \\n\\uf0b7 Aliases & Joins  \\n \\nModule 4  \\n\\uf0b7 Union, Concat & Count  \\n\\uf0b7 Using The IN Clause  \\n\\uf0b7 Math & Subqueries  \\n\\uf0b7 Using Group By  \\n \\nModule 5  \\n\\uf0b7 DB Design  \\n\\uf0b7 Finalize Database Model  \\n\\uf0b7 Generate the Physical Database Schema  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ce86d579-d52b-45dd-ad05-ac3c0ee4c016', embedding=None, metadata={'page_label': '19', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"9.  MTH 151  Calculus   3 Cr  \\n \\nThe first course in Calculus dealing with functions, limits, derivatives, application of \\ndifferentiation, integral calculus and applicati on of integration.  \\n \\nCO1:  To learn the basic information about functions with limits and continuity  \\nCO2:  To understand the ideas of derivatives, higher order derivative and associated rules  \\nCO3:  To describe the concepts and applications of derivatives and  higher order derivatives  \\nCO4:  To acquire the basic ideas of integration  \\nCO5:  To apply the techniques of integral to various problems of finding length of plane curves and  \\narea between the curves  \\n \\nModule 1 : Functions   \\n\\uf0b7 Representing functions  \\n\\uf0b7 Mathematical models  \\n\\uf0b7 Generating new functions  \\n \\nModule 2: Limits  \\n\\uf0b7 Rates of change, velocity, and tangents  \\n\\uf0b7 The limit of a function   \\n\\uf0b7 Limit laws  \\n\\uf0b7 d.Continuity  \\n \\nModule 3 Derivatives  \\n\\uf0b7 Derivatives and rates of change  \\n\\uf0b7 The derivative as a function  \\n\\uf0b7 Differentiation f ormulae  \\n\\uf0b7 Derivatives of trigonometric functions  \\n\\uf0b7 The chain rule  \\n\\uf0b7 Implicit Differentiation  \\n\\uf0b7 Related Rates  \\n\\uf0b7 Linear Approximations and Differentials  \\n\\uf0b7 Applications to other fields  \\n \\nModule 4: Applications of Differentiation   \\n\\uf0b7 Maximum and minimum values  \\n\\uf0b7 The mean value  theorem  \\n\\uf0b7 Curve sketching  \\n\\uf0b7 Optimization  \\n\\uf0b7 Newton's method  \\n\\uf0b7 Antiderivatives  \\n \\nModule 5: Integration   \\n\\uf0b7 Area and distances  \\n\\uf0b7 The definite integral  \\n\\uf0b7 The Fundamental Theorem of Calculus  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='68eca0ff-78f2-4a13-9b3a-84bc0037a833', embedding=None, metadata={'page_label': '20', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Indefinite Integrals  \\n\\uf0b7 Substitution  \\n \\nModule 6 Application of Integration  \\n \\n \\n \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='524fecca-3bea-4714-8d6d-50d61bb32569', embedding=None, metadata={'page_label': '21', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10. CSE  180 Advanced Python Programming   3 Cr  \\n \\nSecond course in Python Programming. The course is approached as the advanced course \\nbased on the prior course on basics in python programming. Topics include Python Distributions, \\nData Sciences and Tools, Data Prepa ration, Data Cleansing, Data Normalization, Data \\nStandardization, Linear Regression,  Logistic regression, Support Vector Machine, Working with Iris \\ndataset, Accuracy Prediction,  K-Mean and K -nearest.  \\n \\nCO1:  Introducing Python IDE’s and Implement Basic Data  Science Concepts, Data Preparation, \\nData Cleansing, Data Normalization, Data Standardization  \\nCO2:  Analyse and Incorporate the various options using Numpy and Pandas to analyse the data  \\nCO3:  Understand the importance of data visualization and the desig n and use of many visual \\ncomponents  \\nCO4:  Demonstrate the various machine learning techniques using the Sklearn package and apply \\nthe concept of unsupervised learning and Clustering for applications and calculate the \\naccuracy score . \\nCO5:  \\n Demonstrate Tensor  Flow Data Structures, Implement Data Capstone Projects with Advance \\nPython Concepts and Machine Learning Capabilities  \\n \\n \\nModule 1  \\n\\uf0b7 Python Distributions  \\n\\uf0b7 Data Sciences and Tools  \\n\\uf0b7 Jupyter Notebook  \\n\\uf0b7 Spyder  \\n\\uf0b7 Data Preparation  \\n\\uf0b7 Data Cleansing  \\n\\uf0b7 Data Normalization  \\n\\uf0b7 Data Standardization  \\nModule 2  \\n\\uf0b7 Python Data Analysis – Numpy  \\n\\uf0b7 Array  \\n\\uf0b7 Indexing  \\n\\uf0b7 Operations  \\n\\uf0b7 Pandas  \\n\\uf0b7 Operations  \\n\\uf0b7 GroupBy  \\n\\uf0b7 Quantitative and Qualitative Data Analysis  \\n\\uf0b7 Pandas Data Structures  \\nModule 3  \\n\\uf0b7 Data Visualization with matplotlib  \\n\\uf0b7 Plotting Curve s  \\n\\uf0b7 Scatter Plot  \\n\\uf0b7 Bar Chart and Histogram  \\n\\uf0b7 Box plot  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='930f0180-f0b8-4d4f-99f9-3db8b60e1b9b', embedding=None, metadata={'page_label': '22', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Pie-chart  \\n\\uf0b7 Data Visualization with Seaborn  \\n\\uf0b7 Built -in Data Visualization  \\n\\uf0b7 Plotly and Cufflinks  \\nModule 4  \\n\\uf0b7 Linear Regression  \\n\\uf0b7 Logistic regression  \\n\\uf0b7 Support Vector Machine  \\n\\uf0b7 Working with Iris dataset  \\n\\uf0b7 Accuracy Prediction  \\n\\uf0b7 K-Mean and K -nearest   \\nModule 5  \\n\\uf0b7 Tensorflow  \\n\\uf0b7 Data Capstone projects  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e51f54b9-bd7d-4733-98cf-4af5c7eed8a5', embedding=None, metadata={'page_label': '23', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11. CSE 150  Web Design - Frontend Development   3 Cr  \\n \\nThe course focuses on the basics of Web design and application.  Starting with the structure of HTML \\ndocument, it leads  to web design and development.  \\n \\nCO1:  Ability to create responsive web pages with multimedia content using HTML 5  \\nCO2:  Acquire skills in developing dynamic web pages using CSS.  \\nCO3:  Ability to create interactive webpages using Typescript and enhance its functionality using Angular \\nand its services.  \\nCO4:  Ability to develop multipurpose and customizable webpages using Bootstrap framework.  \\nCO5:  Design a responsive webpage with Authentication and Authorization.  \\n \\nModule 1  \\n\\uf0b7 Structure of HTML document  \\n\\uf0b7 HTML Basics  \\n\\uf0b7 Form and Form inputs  \\n\\uf0b7 Tables and Lists  \\n\\uf0b7 Multimedia Tags  \\n\\uf0b7 HTML5 Graphics  \\n \\nModule 2  \\n\\uf0b7 Introduction to CSS  \\n\\uf0b7 Introduction to Bootstrap  \\n\\uf0b7 Grid Systems  \\n\\uf0b7 Tables  \\n\\uf0b7 Alerts, Button and Button groups  \\n\\uf0b7 Inputs  \\n\\uf0b7 Accordian and Tab Menu  \\n\\uf0b7 Glyphicons  \\n\\uf0b7 Tooltip  \\n\\uf0b7 Model Pop ups  \\n \\nModule 3  \\n\\uf0b7 Introduction to Angular Basics  \\n\\uf0b7 First Angular APP  \\n\\uf0b7 Introduction to Typescript  \\n\\uf0b7 Building Blocks of Angular  \\n\\uf0b7 Components  \\n\\uf0b7 Directives  \\n\\uf0b7 Services  \\n\\uf0b7 Dependency Injection  \\n \\nModule 4  \\n\\uf0b7 Bootstrap integration  \\n\\uf0b7 Bindings  \\n\\uf0b7 Building reusable components  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6f25a5f7-7219-4feb-aabc-9eeb04443385', embedding=None, metadata={'page_label': '24', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Custom Directives  \\n\\uf0b7 Consuming Http Services  \\n\\uf0b7 Routing and Navigations  \\n \\nModule 5  \\n\\uf0b7 Authentication and Authorization  \\n\\uf0b7 Design Project  \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d6775975-ac8d-418a-b37c-2631444947e1', embedding=None, metadata={'page_label': '25', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12. PHY 145  Physics   3 Cr  \\nThis is an introductory engineering physics course. It deals with electricity, magnetism, electronics, \\nlaws of mechanics and thermodynamics.  \\nCO1: To understand the concepts of Electric fields, Potentials, and Electric circuits with practical \\nknowledge  CO2: To acquire  the knowledge on concept of Magnetic forces, Fields, Capacitance and \\nElectromagnetic induction  CO3: To understand the characteristic of semi -conductors and its application through basic theories with \\nPractical knowledge  CO4: To know the application of Mechanical laws, and forces.  \\nCO5:  To gain the concepts of Thermodynamics and fluids with Practical applications  \\nModule 1 : Electric  Forces and Fields  \\nElectric Forces and Fields  \\nElectric Potential  \\nGauss's Law  \\nElectric Work and Energy  \\nElectric Current and Circuits  \\n \\nModule 2   Magnetic Forces and Fields  \\n \\nMagnetic Forces and Fields  \\nCapacitors and Dielectrics  \\nMagnetic Interactions and Field  \\n \\nModule 3  Electric Circuits and Electronics  \\nModule 4  Mechanics  \\nMotion and Vectors  \\nForces a nd Newton's Laws  \\nCircular and Rotational Motion  \\nEquilibrium and Elasticity  \\nMomentum and Collisions  \\nEnergy and Work  \\n \\nModule 5: Thermodynamics and Fluids  \\n \\nThermodynamics  \\nFluids  \\n \\n \\n \\n \\n \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='556015d0-9417-45d3-9434-a3510ae622f0', embedding=None, metadata={'page_label': '26', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"13. INT 200  INTERNSHIP    6 Cr \\n \\nCO1  Use knowledge and skills acquired in science and engineering to explore and critically \\nanalyse a real -world problem; use creativity and innovation to identify and critically \\nevaluate potential solutions to the problem; choose and implement a particular \\nsolution ( Problem statement, Methodology, Innovation, Project comprehension)  \\nCO2  Design, implement and evaluate a software system to meet desired needs within \\nrelevant constraints; evaluate and analyse the outcomes and impact of the system                 \\n( Design, Implementation & Broader impact)  \\nCO3  Use of modern techniques and tools, communication tools, standard documentation \\nand testing practices (Usage of modern tool/technology)  \\nCO4  Conduct experiments to test software systems and interpret results to debug and \\nimprove system; achieve the proposed solution  (Real time Testing & \\nResult/Outcome)  \\nCO5  Work effectively toward a common objective in software development teams and \\ncontribute effectively; Apply verbal and written communication skills to expla in \\ntechnical problem solving techniques and solutions to global audience (Each \\nStudents' contribution, Communication , Presentation  content)  \\nCO6  Exhibit independent learning and professional development and practice life -long \\nlearning (Learning from reso urces & Future scope)  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e9304841-3b7a-4676-9594-d81f348c3046', embedding=None, metadata={'page_label': '27', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.  CSE  210   IoT- 2      3 Cr  \\n \\nThis is the second course in Internet of Things. It builds on the concepts learned in Python \\nProgramming and the Use of Arduino Micro controllers in the first course. The course focuses on \\nRasberry Pi as the hardware tool to design and build IOTs. Python i s a mandatory background to \\nprogram, control and network the system built in IOT -2.  \\n \\nCO1:  Understand the basic concepts of IoT and get familiar with the features of Raspberry Pi  \\nCO2:  Experiment the network setup and configurations in headless mode of operation by learning \\nvarious remote addressing methods  \\nCO3:  Learn OS basics, code and interpret Linux terminal commands for Raspberry Pi  \\nCO4:  Develop project using Python -Linux terminal command code for controlling the components \\nconnected to Raspberry Pi and to realize the application of Raspberry Pi to access SMTP \\ninbox, to install webserver and to manipulate GPIO pins  \\nCO5:  Build home automation projects using Raspberry Pi by interfacing electronic components and \\nto learn the blog hosting with Raspber ry Pi  \\n \\n \\nModule 1  \\n\\uf0b7 Introduction to Raspberry Pi  \\n\\uf0b7 Hardware Description & Pin Configuration  \\n\\uf0b7 Raspberry Pi B+  \\nPreparing SD card for installation and configuration  \\n \\nModule 2  \\n\\uf0b7 Network Setup  \\n\\uf0b7 GPIO Setup  \\n\\uf0b7 Pi Using Secure Shell (SSH)  \\n\\uf0b7 Pi over VNC  \\nModule 3  \\n\\uf0b7 OS Basics  \\n\\uf0b7 Linux  Basics  \\n\\uf0b7 Linux Commands  \\n\\uf0b7 File Structure and Permissions  \\n\\uf0b7  \\nModule 4  \\n\\uf0b7 Python for Raspberry Pi  \\n\\uf0b7 Accessing SMTP inbox using Python  \\n\\uf0b7 Manipulating GPIO pins using Python  \\n\\uf0b7 Making Raspberry Pi as a Web Server  \\nModule 5  \\n\\uf0b7 Hosting a blog on Raspberry Pi  \\n\\uf0b7 Interfacing Electronic  Component on Raspberry – Pi \\n\\uf0b7 Home Automation using Raspberry - Pi \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='25ec33e9-c001-4281-9502-6885764ee87c', embedding=None, metadata={'page_label': '28', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.  CSE 220  Web Programming and Scripting  3 Cr  \\n \\nCO1:  Create effective scripts using JavaScript to enhance the end user experience.  \\nCO2:  Describe and utilize JavaScript (ES6) advanced programming concepts such as Arrow \\nfunctions, Template literals, etc.  \\nCO3:  Demonstrate Knowledge of Node Package Manager and Mongo DB concepts.  \\nCO4:  Implement web applications using Express JS framework.  \\nCO5:  Test, debug, and deploy JavaScript -based web  applications.  \\n \\n \\nModule 1 Introduction – Java Script (JS)  \\n \\n\\uf0b7 Software Setup  \\n\\uf0b7 JS - Introduction  \\n\\uf0b7 JS - Types and Variables  \\n\\uf0b7 JS - Objects  \\n\\uf0b7 JS - Functions  \\n\\uf0b7 JS - Operators  \\n\\uf0b7 JS - Expressions vs Statements  \\n\\uf0b7 JS - Scopes  \\n\\uf0b7 JS - Loops and Conditional Statements  \\n \\nModule 2 JS - Advanced Topics  \\n \\n\\uf0b7 Variables Lifecycles  \\n\\uf0b7 ES6 Arrow functions  \\n\\uf0b7 ES5.1 Array Helper Methods  \\n\\uf0b7 ES6 Template Literals  \\n\\uf0b7 ES6 Rest/Spread Operators and Default Function Parameters  \\n\\uf0b7 ES6 Enhanced Object Literals  \\n\\uf0b7 ES6 Array and Object Destructuring  \\n\\uf0b7 ES6 Classes, Prototypes and Function Constructors  \\n\\uf0b7 Babel Introduction  \\n\\uf0b7 NPM - Node Package Manager  \\n\\uf0b7 Webpack  \\n\\uf0b7 Introduction to the MongoDB  \\n \\nModule 3 Express JS  \\n \\n\\uf0b7 Introduction to JS  \\n\\uf0b7 Routing  \\n\\uf0b7 Template Engines  \\n\\uf0b7 Middleware  \\n\\uf0b7 Web App Components  \\n\\uf0b7 Integrating a Database  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='37e08f52-adb6-4ca2-a31e-d07b4fdea070', embedding=None, metadata={'page_label': '29', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Error Handling, Debugging, Se curity, and Optimization  \\n\\uf0b7 Proper App Structure  \\n\\uf0b7 Creating an Embedded Map App  \\n\\uf0b7 Creating a Chat App  \\n \\nReferences  \\n \\nhttps://www.udemy.com/course/javascript -bible/  \\nhttps://www.udemy.com/course/expressjs -from -beginner -to-advanced/  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5ce60581-2cf2-4731-9abe-5490c100de03', embedding=None, metadata={'page_label': '30', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.  MTH 271   Linear Algebra   3 Cr  \\nCO1  To analyze the solution set of system of linear equations using matrices  \\nCO2  \\n To generalize the concepts of a real (complex) vector space to an arbitrary finite -dimensional \\nvector space  \\nCO3  To investigate properties of vector spaces and subspaces using by linear transformations  \\nCO4  To acquire knowledge of matrix diagonalization using eigen values and eigenvectors  \\nCO5  To study about scalar product and normalization of vectors using inner products  \\nModule 1  \\nFields, Systems of Linear Equations, Matrices and Elementary Row Operations, Row -Reduced \\nEchelon Matrices, Matrix Multiplication, Invertible Matrices  \\nModule 2  \\nVector spaces (VS), subspaces, linear independence, span, column space, row space, null space, basis, \\ndimension.  \\nModule 3  \\nLinear transformations, matrix of a linear transform ation, null spaces and ranges, Invertibility, rank-\\nnullity theorem and its applications, change of basis, similarity, determinants, transpose.  \\nModule 4  \\nEigenvalues, eigenvectors, characteristic polynomials, minimal polynomials, Cayley -Hamilton \\ntheorem, algebraic multiplicity, geometric multiplicity, diagonalization.  \\n \\nModule 5  \\nInner products, inner product spaces, linear functionals and adjoints, unitary and normal operators, \\nGram -Schmidt process. Projections, least squares for model -fitting and applications.  \\n \\nReferences:  \\n\\uf0b7 Sheldon Axler , Linear algebra done right, Springer publications.  \\n\\uf0b7 David C. Lay, Steven R Lay and Judi J. McDonald , Linear Algebra and Its \\nApplications , Fifth edition,  Pearson education limited.  \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f8f4e7c6-8386-4c76-9c7b-76bbc136a41f', embedding=None, metadata={'page_label': '31', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.  CSE 250    Operating Systems   3 Cr  \\n \\nCO1:  To impart a thorough understanding and the evolution of the operating system and how the \\noperating system manages the hardware devices and coordinates them.  \\nCO2:  To understand and gain knowledge on CPU: Process scheduling and synchronization  \\nCO3:  To analyze the various memory hierarchy and able to calculate the access times  \\nCO4:  To understand the various concepts involved in file system management  \\nCO5:  To implement and understand OS concepts through a few case studies  \\n \\n \\nModule 1 : Fundamentals of OS  \\n\\uf0b7 Overview of operating systems functionalities  \\n\\uf0b7 Hardware concepts related to OS  \\n\\uf0b7 CPU states  \\n\\uf0b7 I/O channels  \\n\\uf0b7 Memory Hierarchy  \\n\\uf0b7 Microprogramming  \\nModule 2: Process  \\n\\uf0b7 The concept of a process  \\n\\uf0b7 Job and processor scheduling  \\n\\uf0b7 Scheduling Algorithms  \\n\\uf0b7 Synchronisation & Deadlock  \\n\\uf0b7 Inter process Communication  \\nModule 3: Memory Management   \\n\\uf0b7 Memory Organisation and Management  \\n\\uf0b7 Storage Allocation  \\n\\uf0b7 Virtual memory concep ts  \\n\\uf0b7 Paging and segmentation  \\n\\uf0b7 Address mapping  \\n\\uf0b7 Virtual Storage Management  \\nModule 4: File Organisation  \\n\\uf0b7 Page replacement strategies.  \\n\\uf0b7 File organisation  \\n\\uf0b7 File and Directory structures  \\n\\uf0b7 File Structure  \\nModule 5 :Case Study  \\n\\uf0b7 Case Study on OS  \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='69f3fcaf-92d5-4cdc-810c-954d861e7240', embedding=None, metadata={'page_label': '32', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References:  \\nhttp://web.stanford.edu/~ouster/cgi -bin/cs140 -spring19/index.php  \\nhttp://web.stanford.edu/~ous ter/cgi -bin/cs140 -spring19/projects.php  \\nhttps://swayam.gov.in/nd1_noc20_cs04/preview  \\nhttp://www.cs.tufts.edu/comp/111/  \\nhttps://www.cs.cmu.edu/~410/expectations.html https://www.cs.cmu.edu/~410/projects.html  \\n \\n \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5ff2946e-327f-4ad3-871f-bf4b6e240a52', embedding=None, metadata={'page_label': '33', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"18.  CSE  270          DESIGN AND ANALYSIS OF ALGORITHMS  3 Cr  \\n \\nCO1:  \\n Ability to analyze the asymptotic performance of algorithms and understand divide and \\nconquer design techniques.  \\nCO2:  Design of algorithmic techniques like backtracking, dynamic programming, greedy, branch \\n& bound, and performance analysis of online, offl ine and randomized algorithms  \\nCO3:  Design of algorithms for network security applications.  \\nCO4:  Design of algorithms for geometric modelling and computer graphic applications.  \\nCO5:  Design of algorithms for parallel and distributed computing based applications.  \\n \\n \\nModule 1: Introduction:  \\nRole of Algorithms in computing, Analysis of Algorithms, Asymptotic notation, Euclid's  algorithm, \\nProblem, Instance, RAM model, Principles of Algorithm Design, Sorting Algorithm - Insertion Sort & \\nComplexity Analysis, Divide and Conquer Technique, Solving recurrences - substitution, Iteration, \\nRecursion tree, Changing variable and Master's Method.  \\n \\nModule 2: Combinatorial Optimization:  \\nBacktracking; Dynamic programming; Greedy Technique; Branch & Bound  \\n \\nModul e 3: Advanced Algorithmic Analysis:  \\nAmortized analysis; Online and offline algorithms; Randomized algorithms,  \\nNP Completeness  \\n \\nModule 4: Cryptographic Algorithms:  \\nHistorical overview of cryptography; Private -key cryptography and the key -exchange  problem;  \\nPublic -key cryptography; Digital signatures; Security protocols; Applications (zero knowle dge proofs, \\nauthentication etc.,  \\n \\nModule 5: Geometric Algorithms:  \\nLine segments: properties, intersections; convex hull finding algorithms, Voronoi Diagram,  Delaunay \\nTriangulation  \\n \\nModule 6 Parallel Algorithms:  \\nPRAM model; Exclusive versus concurrent reads and writes; Pointer jumping; Brent’s  theorem and \\nwork efficiency  \\n \\nModule 7   Distributed Algorithms:  \\nConsensus and election; Termination detection; Fault tolerance; Stabilization  \\n \\n \\nReferences:  \\nhttps://online.stanford.edu/courses/cs161 -design -and-analysis -algorithms  \\nhttps://www.cse.iitm.ac.in/course_details.php?arg=OTI=  \\nhttps://www.cs.ox.ac.uk/teaching/courses/2018 -2019/alg design/  \\nhttps://www.athabascau.ca/syllabi/comp/comp372.php  \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2725faad-18a9-4663-b18b-154040cbf75f', embedding=None, metadata={'page_label': '34', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.  CSE   290   Computer Networks   3 Cr  \\n \\nCO1:  To visualize the different aspects of networks, protocols and network design models.  \\nCO2:  To examine various Data Link layer design issues and Data Link protocols.  \\nCO3:  To analyze and compare different LAN protocols.  \\nCO4:  To compare and select appropriate routing algorithms for a network, outline the mechanisms  \\ninvolved in transport layer.  \\nCO5:  To examine the important aspects and functions of network layer, transport layer and \\napplication layer in internetworking.  \\n \\n \\nModule 1: Basics of Networks  \\n\\uf0b7 Topologies  \\n\\uf0b7 Layering and protocols  \\n\\uf0b7 Internet Architecture  \\n\\uf0b7 Network Tools  \\nModule 2 : Data Link Layer  \\n\\uf0b7 Link layer Services  \\n\\uf0b7 Framing  \\n\\uf0b7 Error Detection  \\n\\uf0b7 Flow control  \\n\\uf0b7 Media access control  \\n\\uf0b7 Ethernet (802.3)  \\n\\uf0b7 Wireless LANs – 802.11  \\n\\uf0b7 Bluetooth  \\nModule 3 : Network Layer  \\n\\uf0b7 IP Addressing  \\n\\uf0b7 Switching and bridging  \\n\\uf0b7 Internetworking  \\n\\uf0b7 Routing  \\n\\uf0b7 Multicast – addresses – multicast routing  \\nModule 4 : Transport Layer  \\n\\uf0b7 UDP / TCP  \\n\\uf0b7 Connection management  \\n\\uf0b7 TCP Congestion control  \\n\\uf0b7 QoS  \\nModule 5: Application Layer  \\n\\uf0b7 Electronic Mail  \\n\\uf0b7 HTTP  Web Services  \\n\\uf0b7 DNS  \\n\\uf0b7 SNMP  \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='35f608a0-6f80-4996-94fb-244db8ce4a28', embedding=None, metadata={'page_label': '35', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References:  \\nhttps://www.cse.wustl.edu/~jain/cis677 -96/toc.html  \\nhttps://www.adelaide.edu.au/course -outlines/002328/1/sem -1/ \\nhttps://www.tru.ca/distance/courses/comp3271.html  \\nhttps://swayam.gov. in/nd2_ugc19_cs10/preview  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bfe81f3b-18a9-4545-bbd5-20e1759f9c95', embedding=None, metadata={'page_label': '36', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.  CSE 2 80  Advanced Java  3 Cr  \\n \\nCO1  Explore the various methods to develop dynamic web applications using JSP.  \\nCO2  Implement the request -response interaction between client and server using servlets.  \\nCO3  Implement and analyze the Java Application Programming Interface to develop \\ncommunication between the client and the database..  \\nCO4:  Implement Plain Old Java Object (POJO) model using spring framework  \\nCO5:  Perform Java Object – Database mapping techniques using Hibernate.  \\n \\nModule 1: JAVA Server Page (JSP)  \\n\\uf0b7 JSP – Introduction, Life cycle of JSP and Directory structure of JSP  \\n\\uf0b7 Implicit object in JSP  \\n\\uf0b7 JSP Scripting Elements  \\no JSP Expressions  \\no JSP Scriptlets  \\no JSP Declarations  \\n\\uf0b7 CRUD in JSP (Create, Read,  Update and Delete)  \\n\\uf0b7 Reading HTML form data with JSP (GET, POST, PUT, DELETE)  \\n \\nModule 2: Servlets  \\n\\uf0b7 Servlet fundamentals – Life Cycle  \\n\\uf0b7 Servlet form data - Reading Servlet parameters  \\n\\uf0b7 Servlet client request and server response  \\n\\uf0b7 Handling cookies  \\n\\uf0b7 Session tracking  \\n\\uf0b7 Page redirect  \\n\\uf0b7 CRUD in Servlet (Create, Read, Update and Delete)  \\n\\uf0b7 Servlet Application:  \\no Registration Example  \\no Uploading/downloading file  \\no Write Data to PDF  \\no Servlet sending mail  \\n \\nModule 3: JDBC and MVC  \\n\\uf0b7 JDBC SQL Syntax and Environment  \\n\\uf0b7 Driver types  \\n\\uf0b7 JDBC Connection  \\n\\uf0b7 JDBC Statements  \\n\\uf0b7 JDBC Result set  \\n\\uf0b7 Introduction to MVC  \\n\\uf0b7 Creating sample web application – JSP, SERVLET, JDBC  \\n \\nModule 4: Spring Framework  \\n\\uf0b7 Spring Framework Architecture  \\n\\uf0b7 Dependency injection  \\n\\uf0b7 IOC container  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='256ca842-ae9e-49c7-9c82-b5de0374ba92', embedding=None, metadata={'page_label': '37', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Spring Modules and uses  \\n\\uf0b7 Spring core - bean creation, life cycle of bean, scope of bean  \\n\\uf0b7 Spring MVC  \\n \\nModule 5 : Hibernate Framework  \\n\\uf0b7 Introduction to ORM  \\n\\uf0b7 Cons in JDBC  \\n\\uf0b7 Hibernate Architecture  \\n\\uf0b7 Hibernate configuration, session, persistent class  \\n\\uf0b7 Mapping files and mapping types  \\n\\uf0b7 Hibernate query languages  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ff795124-864d-486d-a296-90792fe6be10', embedding=None, metadata={'page_label': '38', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='21.  CSE  240       Data Science with R   3 Cr  \\n \\nCO1:  Understand the fundamental syntax of R through practice exercises, demonstrations, and \\nwriting R code  \\nCO2:  Explore various operators, implement Control Structures and Functions for R Programs.  \\nCO3:  Visualizing data using R with a different type of graphs and charts  \\nCO4:  Analyzing and testing the data using statistical techniques like ANOVA, Time series, and \\nHypothesis test  \\nCO5:  Ability to understand and apply to scale up machine learning techniques and associated \\ncomputing techniques and technologies.  \\n \\nModule 1 R basics  \\n\\uf0b7 Introduction to R: R console, Packages, Libraries, and Workspace  \\n\\uf0b7 Data Types: Variables, and Strings, Modifyin g data types  \\n\\uf0b7 Data structures in R: Vectors, Factors, Vector operations, Arrays, Matrices, Lists, and Data \\nframes  \\n\\uf0b7 Programming fundamentals: Conditions and loops, Functions in R, Objects and Classes, \\nDebugging  \\nModule 2 Data, functions and control structures in R \\n\\uf0b7 Reading CSV, Excel Files, text files  \\n\\uf0b7 Writing and saving data objects to file in R  \\n\\uf0b7 Preparing Data in R: Data Cleaning, Tidy Data Concepts, Data imputation and Data \\nconversion  \\n\\uf0b7 String operations in R  \\n\\uf0b7 Regular Expressions  \\n\\uf0b7 Dates in R  \\n\\uf0b7 Working with Built in functions, Working with User -defined functions  \\n\\uf0b7 If statement,  If..else statement, Nested if..else  \\n\\uf0b7 Switch structure  \\n\\uf0b7 For loop, While loop, Repeat loop  \\nModule 3 Data Visualization  \\n\\uf0b7 Bar/pie/line chart/histogram/boxplot/scatter/density  \\n\\uf0b7 Using Base Package, Lattice  Package, ggplot2 package  \\n\\uf0b7 Combining Plots  \\n\\uf0b7 Analysis with ScatterPlot, BoxPlot, Histograms, Pie Charts  \\n\\uf0b7 Set.Seed Function , Preparing Data for Plotting  \\n\\uf0b7 QPlot, ViolinPlot  \\nModule 4 Statistical Analysis Using R  \\n\\uf0b7 Data Summarization: Measures of Centre, Measures of Dispersion, Skewness and Kurtosis  \\n\\uf0b7 Probability Distribution Using R  \\n\\uf0b7 Hypothesis testing: t - Tests  \\n\\uf0b7 Analysis of Variance: One -way ANOVA, Two -way ANOVA  \\n\\uf0b7 Time series data and their graphical representation  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='00ea903d-dc3b-49aa-b9d3-86364fd56e19', embedding=None, metadata={'page_label': '39', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Module 5 Data Analytics Using R  \\n\\uf0b7 Correlation Analysis  \\n\\uf0b7 Simple Linear Regression  \\n\\uf0b7 Multiple linear regression  \\n\\uf0b7 Unsupervised Learning using k -means clustering  \\n\\uf0b7 Supervised Learning - Classification  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ff164300-ca9c-424b-8cbb-eb2725c17c06', embedding=None, metadata={'page_label': '40', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n \\n22.  INT 300  INTERNSHIP  2 – YEAR 2  \\n \\nCO1  Use knowledge and skills acquired in science and engineering to explore and critically analyse \\na real -world problem; use creativity and innovation to identify and critically evaluate potential \\nsolutions to the problem; choose and implement a particular solution  \\n ( Problem statement, Methodolo gy, Innovation, Project comprehension)  \\nCO2  Apply conceptual knowledge for practical implementation of real time industry projects; \\nDesign, implement and evaluate a software system to meet desired needs within relevant \\nconstraints; evaluate and analyse the outcomes and impact of the system                 \\n ( Design, Implementation & Broader impact)  \\nCO3  Use of modern techniques and tools, communication tools, standard documentation and testing \\npractices (Usage of modern tool/technology)  \\nCO4  Conduct experiments to test software systems and interpret results to debug and improve \\nsystem; Enhance the problem solving skills and achieve the proposed solution   \\n(Real time Testing & Result/Outcome)  \\nCO5  Work effectively toward a common objective in software development teams and contribute \\neffectively; Apply verbal and written communication skills to explain technical problem \\nsolving techniques and solutions to global audience  \\n(Each Students' contribution, Communication , Presentation  content)  \\nCO6  Exhibit independent learning and professional development an d practice life -long learning \\n(Learning from resources & Future scope)  \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9c8dfcf0-ec9a-4907-a5a2-8808cacaff73', embedding=None, metadata={'page_label': '41', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='23. ENG 226 – English 2                                                              3 Cr  \\nCO1  Enhanced vocabulary skills and proficiency in English language  \\nCO2  Enhanced comprehension of native English speaking and ability to clear TOEFL Listening \\nmodule  \\nCO3  Enhanced professional English speaking skills and ability to clear TOEFL Speaking module  \\nCO4  Enhanced reading comprehension of everyday English and ability to clear TOEFL Reading \\nmodule  \\nCO5  Enhanced professional English writing skills and ability to clear TOEFL Writing module  \\n \\nUnit 1 – LANGUAGE PROFICIENCY  \\nSuffixes and Prefixes -Synonyms and Antonyms – Homonyms, Homophones, Homographs – \\nCollocation -Connotation - Words  often Confused - One Word Substitution - Idioms and Phrasal Verbs - \\nExercises  \\n \\nUnit 2– LISTENING  \\nProsodic Features: Stress, Intonation, Pause, Pitch etc. – Advanced Listening Exercises: messages  \\nand announcements, Songs, conversation with a native speaker et c. – TOEFL Listening: gist -content  \\nand gist -purpose questions, detail questions, function questions, attitude questions, organization  \\nquestions, connecting content questions, inference questions.  \\n \\nUnit 3 – SPEAKING  \\nBusiness Etiquette: Introduction and Greeting, Business Meetings, Oral Presentations, Telephonic  \\nInterviews – TOEFL Speaking: Independent Task, Integrated Task: Read, Listen and Speak, Listen  \\nand Speak  \\n \\nUnit 4 – READING  \\nTypes of Reading: Skimming, Scanning, Reading for Understanding, Critical  Reading – Newspaper  \\nReading – Note Making – Cloze tests – Advanced Reading Exercises – TOEFL Reading: factual  \\ninformation and negative factual information, Inference and rhetorical purpose questions, vocabulary  \\nquestions, sentence simplification questions , insert text questions, prose summary and fill in a table  \\nquestions  \\n \\nUnit 5 – WRITING  \\nPrecise - Developing Outline - Email - Notice, Agenda, Minutes - Creative Writing: Newspaper Story  \\nWriting, Movie Script Writing – TOEFL Writing: Integrated Task, Independen t Task  \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0a1544cf-3a44-43ef-8b52-b0ab0abbbeea', embedding=None, metadata={'page_label': '42', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24. CSE 230 Mobile Programming                                                  3 Cr  \\n \\nCO1  Demonstrate the Understanding of fundamentals of Android Programming.  \\nCO2  Build their ability to apply various components in mobile application development.  \\nCO3  Ability to develop user interactions with lists and views  \\nCO4  Discover the life cycles of Activities, Applications, intents, and fragments.  \\nCO5  Design, develop case studies and publish them in Google play  \\n \\nModule 1: Fundamentals of Mobile Programming  \\n\\uf0b7 Introduction to Android Application Development  \\n\\uf0b7 Android Ecosystem  \\n\\uf0b7 Android Studio  \\n\\uf0b7 Emulators  \\n\\uf0b7 Gradle Build System  \\n\\uf0b7 Manifest File  \\n\\uf0b7 Introduction to Resources (Strings, Drawables etc)  \\n\\uf0b7 The R.java file  \\n \\nModule 2: Android Components  \\n\\uf0b7 Layouts  \\n\\uf0b7 Text views  \\n\\uf0b7 Buttons  \\n\\uf0b7 Edit texts  \\n\\uf0b7 Image View  \\n\\uf0b7 Checkbox  \\n\\uf0b7 Radio Buttons  \\n\\uf0b7 Toggle Buttons  \\n\\uf0b7 Spinner  \\n \\nModule 3: User Interactions  \\n\\uf0b7 Toast Messages  \\n\\uf0b7 Snack bar Messages  \\n\\uf0b7 Dialog Messages  \\n\\uf0b7 Constraint Layout  \\n\\uf0b7 List View  \\n\\uf0b7 Recycler View  \\n\\uf0b7 Grid View  \\n\\uf0b7 Scroll View  \\n\\uf0b7 WebView  \\n \\nModule 4: Lifecycles &  Shared Preferences  \\n\\uf0b7 Application Lifecycle  \\n\\uf0b7 Activity & Lifecycle  \\n\\uf0b7 Fragment & Lifecycle  \\n\\uf0b7 Services  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='690a2042-3281-4803-b67d-8cda99b6067a', embedding=None, metadata={'page_label': '43', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"\\uf0b7 Receivers  \\n\\uf0b7 Intents  \\n\\uf0b7 Shared Preferences Class  \\n\\uf0b7 Saving Data Local Memory  \\n\\uf0b7 Calling Back Data  \\n \\nModule 5: Case studies & Publishing App on Google Play  \\n\\uf0b7 Case studies  \\n\\uf0b7 APK Release Version  \\n\\uf0b7 APK files and its types  \\n\\uf0b7 Google Play Developer Account  \\n\\uf0b7 Release Your App  \\n \\nReferences:  \\n1. https://www.udemy.com/course/full -stack -android -development -and-mobile -app-marketing/  \\n2. https://www.udemy.com/course/mobile -app-development -with-android -2015/  \\nReference Books:  \\n1. Android: A Programming Guide by J.F. DiMarzio.  \\n2. Hello, Android: Introducing Google's Mobile Development Platform by Ed Burnett.  \\n3. Programmi ng android by Zigurd Mednieks.  \\n4. Android User Interface Design: Turning Ideas and Sketches into Beautifully Designed  \\nApps byIan G. Clifton.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='262fa1ac-936b-4b31-be97-3738ac80a369', embedding=None, metadata={'page_label': '44', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='25. CSE 260 Data Visualization Tools and Techniques                                        3 Cr \\nCO1  Understand the importance of data visualization and demonstrate the ability to choose \\nthe right tool and technique for the given business problem  \\nCO2  Demonstrate the ability to use Tableau software to analyze and visualize time series \\ndata \\nCO3  Design and develop tableau applications using maps and understand the basics of \\ncolors, views, and other critical visualization -based issues  \\nCO4  Demonstrate the ability to use Tableau software to plan, design, layout and implement \\ndashboards and stories.  \\nCO5  Design and implement advanced dashboards using Tableau  \\n \\nModule 1 Data Visualization – Introduction to Tableau  \\nOverview of different types of data visualization tools and their features  \\nChoosing the right visual and the right tool  \\nFactors that influence the choice of right tool  \\nTypes of data visualization techniques and choosing the right technique  \\nInstallation and Use of Tableau  \\n \\nModule 2 Tableau  \\nUnderstanding the layout and the concept of cards / shelves  \\nCreating, formatting and displaying bas ic charts  \\nCreating multiple types of charts (including maps, heat maps and tree maps)  \\nUsing trend lines to analyze  and display time series data  \\nWorking with data hierarchies  \\nUsing filters, sets, groups and hierarchies to enhance views  \\n \\nModule 3 Tableau Application  \\nVisual Analysis  \\nMaps and Locations  \\nColor Vision Deficiencies  \\nCalculations  \\n \\nModule 4 Data Visualization Presentation: Dashboards and Storytelling  \\nCreative data visualization  \\nSelection and presentation of key performance indicators (KPIs)  \\nElement s of graphic design for visualization and project web site design  \\nCreating interactive dashboards using techniques like filtering and highlighting  \\nDashboard planning, design, layout and implementation  \\nIncorporating multimedia objects  \\nMethods for animating data visualizations to illustrate change  \\nDeveloping effective and engaging data stories  \\nDecision making using data visualization  \\n \\nModule 5  \\nAdvanced Dashboards  \\nDashboard Applications  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='08b8b3d5-21bc-45e2-b298-ed5cca37a282', embedding=None, metadata={'page_label': '45', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Visualization Certification  \\n \\nReferences  \\nhttps://www.udemy.com/course/ complete -tableau -bootcamp -dashboards/  \\nhttps://www.udemy.com/course/tableau -2018 -tableau -10-qualified -associate -certification/  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8313c03e-c941-4051-be5c-ddb077c0f49d', embedding=None, metadata={'page_label': '46', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26. MTH 221 Differential Equations                                                            3 Cr \\nCO1  Understanding types of differential equations and solving first order differential equations  \\nCO2  Solving second order differential equations and dealing with applications  \\nCO3  Solving PDE and using PDE as a tool to solve real -world applications  \\nCO4  Learning various properties of Laplace and Inverse Laplace transforms to solve DE  \\nCO5  Enabling the students to learn various properties of Fourier tra nsform  \\n \\nModule 1 - First order Differential Equations  \\n\\uf0b7 Introduction to differential equations  \\n\\uf0b7 Order and degree  \\n\\uf0b7 Homogeneous equations and Non -homogeneous equations  \\n\\uf0b7 Solution by separating variables  \\n\\uf0b7 Exact equations  \\n\\uf0b7 Solving first order linear differential equations  \\n\\uf0b7 Bernoulli’s Equation  \\n \\nModule 2 – Second order Differential Equations  \\n\\uf0b7 Solving second order differential equations with constant coefficients  \\n\\uf0b7 Homogenous solutions  \\n\\uf0b7 Non-homogeneous solutions  \\n\\uf0b7 Superposition Principle  \\n\\uf0b7 Initial value problems(IVP)  \\n\\uf0b7 Wronskian  \\n \\nModule 3 – Partial Differential Equations  \\n\\uf0b7 Introduction  \\n\\uf0b7 Equations solvable by direct integration  \\n\\uf0b7 Linear equations of the first order  \\n\\uf0b7 Lagrange’s linear equation  \\n\\uf0b7 Solution of equation by method of separation of variable  \\n \\nModule 4 - Laplace Transforms  \\n\\uf0b7 Introduction  \\n\\uf0b7 Elementary Properties  \\n\\uf0b7 Derivative Property of the Laplace Transform  \\n\\uf0b7 Inverse Laplace Transform  \\n\\uf0b7 Solving IVP using Laplace transform  \\n \\nModule 5 -Fourier Transforms  \\n\\uf0b7 Statement of Fourier integral theorem  \\n\\uf0b7 Fourier transform pair  \\n\\uf0b7 Fourier sine  and cosine transforms  \\n\\uf0b7 Properties  \\n\\uf0b7 Transforms of simple functions  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='745860ce-4b8b-4d16-83f5-4669f9222159', embedding=None, metadata={'page_label': '47', 'file_name': 'B.Tech I & II Year CO with syllabus.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/B.Tech I & II Year CO with syllabus.pdf', 'file_type': 'application/pdf', 'file_size': 1169819, 'creation_date': '2024-09-21', 'last_modified_date': '2024-09-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='\\uf0b7 Convolution theorem  \\n\\uf0b7 Parseval’s identity.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='de732c79-d546-451b-b126-b29d4fb4a457', embedding=None, metadata={'page_label': '1', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='53d4ff13-668b-4e91-94eb-96e9f701ea65', embedding=None, metadata={'page_label': '2', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='25d2973b-3a10-4da0-8acd-c1ad2dec926a', embedding=None, metadata={'page_label': '3', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fc30d9ab-9c7e-48e3-94f6-8244e504f373', embedding=None, metadata={'page_label': '4', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d8747a3b-1bf1-4c60-9879-405fbf1c2cb7', embedding=None, metadata={'page_label': '5', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='27027c7c-c4d3-462b-8769-d456b568e5cd', embedding=None, metadata={'page_label': '6', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d102f0e9-b42a-4d0a-b7dd-1d8d61183d97', embedding=None, metadata={'page_label': '7', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='368a3d38-5fb5-40fe-a560-a3f976a7e379', embedding=None, metadata={'page_label': '8', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='49c8c24e-2e94-430d-9fd5-1d7b2088648b', embedding=None, metadata={'page_label': '9', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6d9368f8-c1a8-448f-80af-e1b1fb2cae99', embedding=None, metadata={'page_label': '10', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6f373eb0-9a06-44b4-b4b8-42af8f96a63a', embedding=None, metadata={'page_label': '11', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8381bdfb-a62a-4812-a051-e7de30ea7117', embedding=None, metadata={'page_label': '12', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bba9ad91-cedc-42cc-b136-954f6c1fc7ce', embedding=None, metadata={'page_label': '13', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3917cfa2-8bf7-4221-b902-d247ce7603fa', embedding=None, metadata={'page_label': '14', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='093b4e2c-c1bc-4299-9c9f-4141d76107e8', embedding=None, metadata={'page_label': '15', 'file_name': 'attention.pdf', 'file_path': '/Users/mehamurali/Desktop/Optiwise/Year 3/Project/data/attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-09-21', 'last_modified_date': '2024-04-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "print(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:50:53.801404Z",
     "start_time": "2024-09-21T12:50:52.358105Z"
    }
   },
   "id": "d6df02175818ccba",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.Tech. Computer Science and Engineering (Artificial Intelligence & Machine \n",
      "Learning)  \n",
      "Revised Scheme and Curriculum  \n",
      "I year Courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 1)  \n",
      "1.  CSE       110 Problem Solving - C Programming  3 \n",
      "2.  CSE       120 Python Programming  3 \n",
      "3.  CSE       130 IOT - 1  3 \n",
      "4.  ENG        126 English I  3 \n",
      "Quarter -II (Year 1)  \n",
      "5.  CSE       160 Data Structures  3 \n",
      "6.  MTH        141 Probability & Statistics  3 \n",
      "7.  CSE       170 Java Basics with OOPs Concepts  3 \n",
      "8.  CSE       140 Database Management Systems  3 \n",
      "Quarter -III (Year 1)  \n",
      "9.  MTH        151 Calculus  3 \n",
      "10.  CSE       180 Advanced Python Programming  3 \n",
      "11.  CSE       150 Web Design – Frontend Development  3 \n",
      "12.  PHY       145 Physics  3 \n",
      "Quarter -IV (Year 1)  \n",
      "13.  INT       200 Internship 1   6 \n",
      "TOTAL  42 \n",
      " \n",
      "II Year Courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 2)  \n",
      "14. CSE       210  IOT - 2 3 \n",
      "15. CSE       220  Web Programming & Scripting  3 \n",
      "16. MTH       271  Linear Algebra  3 \n",
      "17. CSE       250  Operating Systems  3 \n",
      "Quarter -II (Year 2)  \n",
      "18. CSE       270  Design and Analysis of Algorithms  3 \n",
      "19. CSE       290  Computer Networks  3 \n",
      "20. CSE       280  Advanced  Java  3 \n",
      "21. CSE       240  Data Science with R  3 \n",
      "Quarter -III (Year 2)  \n",
      "22. INT       300  Internship 2  6 \n",
      " \n",
      "Quarter -IV (Year 2)  \n",
      "23. CSE       230  Mobile Programming  3 \n",
      "24. MTH       221  Differential Equations  3 \n",
      "25. CSE       260  Data Visualization Tools and Techniques  3 \n",
      "26. ENG       226  English -2 3 \n",
      "TOTAL  42 \n",
      "********************************************************************************\n",
      "III Year courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 3)  \n",
      "27. INT       400  Internship 3  6 \n",
      "Quarter -II (Year 3)  \n",
      "28. MTH       351  Advanced Statistics  3 \n",
      "29. CSE       330  Cloud Computing  3 \n",
      "30. CSE       310  Artificial Intelligence and Machine Learning  3 \n",
      "31. CSE       320 Data Mining  3 \n",
      "Quarter -III (Year 3)  \n",
      "32. CSE       340  Machine Learning 1  3 \n",
      "33. CSE       370  Cyber Security  3 \n",
      "34. MTH       331  Discrete Mathematics  3 \n",
      "35. CSE       360  Deep Learning  3 \n",
      "Quarter -IV (Year 3)  \n",
      "36. CSE       345  Elective I – Block Chain Technologies  3 \n",
      "37. CSE       380  Natural Language Processing  3 \n",
      "38. CHE       305  Chemistry  3 \n",
      "39. CSE       355  Elective II - Data Science Professional \n",
      "Certification      3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "IV Year Courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 4)  \n",
      "40. CSE       440  Reinforcement Learning  3 \n",
      "41.  Elective III  3 \n",
      "42. CSE       410  Computer Vision  3 \n",
      "43.  Elective IV  3 \n",
      "Quarter -II (Year 4)  \n",
      "44. CSE       420  Software Engineering & Testing  3 \n",
      "45.  Elective V  3 \n",
      "46.  Elective  VI 3 \n",
      "47. HUM       406  Humanities Course  3 \n",
      "Quarter -III & IV  (Year 4)  \n",
      "48. INT       500  Internship 4  6 \n",
      "49.  INT       510  Internship 5  6 \n",
      "TOTAL  36 \n",
      " \n",
      " \n",
      "TOTAL  CREDITS : 162  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "B.Tech. Computer Science and Engineering (Cybersecurity &  Internet of Things)  \n",
      "Revised Scheme and Curriculum  \n",
      " \n",
      "I year Courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 1)  \n",
      "1.  CSE       110 Problem Solving - C Programming  3 \n",
      "2.  CSE       120 Python Programming  3 \n",
      "3.  CSE       130 IOT - 1  3 \n",
      "4.  ENG        126 English I  3 \n",
      "Quarter -II (Year 1)  \n",
      "5.  CSE       160 Data Structures  3 \n",
      "6.  MTH        141 Probability & Statistics  3 \n",
      "7.  CSE       170 Java Basics with OOPs Concepts  3 \n",
      "8.  CSE       140 Database Management Systems  3 \n",
      "Quarter -III (Year 1)  \n",
      "9.  MTH        151 Calculus  3 \n",
      "10.  CSE       180 Advanced Python Programming  3 \n",
      "11.  CSE       150 Web Design – Frontend Development  3 \n",
      "12.  PHY       145 Physics  3 \n",
      "Quarter -IV (Year 1)  \n",
      "13.  INT       200 Internship 1   6 \n",
      "TOTAL  42 \n",
      " \n",
      "II Year Courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 2)  \n",
      "14. CSE       210  IOT - 2 3 \n",
      "15. CSE       220  Web Programming & Scripting  3 \n",
      "16. MTH       271  Linear Algebra  3 \n",
      "17. CSE       250  Operating Systems  3 \n",
      "Quarter -II (Year 2)  \n",
      "18. CSE       270  Design and Analysis of Algorithms  3 \n",
      "19. CSE       290  Computer Networks  3 \n",
      "20. CSE       280  Advanced  Java  3 \n",
      "21. CSE       240  Data Science with R  3 \n",
      "Quarter -III (Year 2)  \n",
      "22. INT       300  Internship 2  6 \n",
      "Quarter -IV (Year 2)  \n",
      "23. CSE       230  Mobile Programming  3 \n",
      "24. MTH       221  Differential Equations  3 \n",
      "25. CSE       260  Data Visualization Tools and Techniques  3 \n",
      "26. ENG       226  English -2 3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "III Year courses  \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 3)  \n",
      "27. INT       400  Internship 3  6 \n",
      "Quarter -II (Year 3)  \n",
      "28. MTH       351  Advanced Statistics  3 \n",
      "29. CSE       330  Cloud Computing  3 \n",
      "30. CSE       310  Artificial Intelligence and Machine Learning  3 \n",
      "31. CSE       320  Data Mining  3 \n",
      "Quarter -III (Year 3)  \n",
      "32. CSE       350 IOT Sensors, Microcontrollers & Signal \n",
      "Processing  3 \n",
      "33. CSE       370 Cyber Security  3 \n",
      "34. CSE       390 Wireless Networks  3 \n",
      "35. MTH       331  Discrete Mathematics  3 \n",
      "Quarter -IV (Year 3)  \n",
      "36. CSE       345  Elective I – Block Chain Technologies  3 \n",
      "37. CSE       372  Advanced Cyber Security  3 \n",
      "38. CHE        305 Chemistry  3 \n",
      "39. CSE       355  Elective II - Data Science Professional \n",
      "Certification      3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "IV Year Courses  \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 4)  \n",
      "40. ELE Elective III  3 \n",
      "41. CSE       430  Design Thinking  3 \n",
      "42. CSE       420  Software Engineering & Testing  3 \n",
      "43. CSE       450  IoT Security  3 \n",
      "Quarter -II (Year 4)  \n",
      "44. ELE Elective IV  3 \n",
      "45. ELE Elective V  3 \n",
      "46. ELE Elective VI  3 \n",
      "47. HUM       406  Humanities Course  3 \n",
      "Quarter -III & IV (Year 4)  \n",
      "48. INT       500  Internship 4  6 \n",
      "49.  INT       510  Internship 5  6 \n",
      "TOTAL  36 \n",
      " \n",
      " \n",
      "TOTAL CREDITS: 162   \n",
      "********************************************************************************\n",
      "B.Tech. Computer Science and Engineering (Artificial Intelligence & Data Analytics)  \n",
      "Revised Scheme and Curriculum  \n",
      " \n",
      "I year Courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 1)  \n",
      "1.  CSE       110 Problem Solving - C Programming  3 \n",
      "2.  CSE       120 Python Programming  3 \n",
      "3.  MTH        141  Probability & Statistics  3 \n",
      "4.  ENG        126 English I  3 \n",
      "Quarter -II (Year 1)  \n",
      "5.  CSE       160 Data Structures  3 \n",
      "6.  CSE       130 IOT - 1 3 \n",
      "7.  CSE       170 Java Basics with OOPs Concepts  3 \n",
      "8.  PHY       145  Physics  3 \n",
      "Quarter -III (Year 1)  \n",
      "9.  MTH        151 Calculus  3 \n",
      "10.  CSE       180 Advanced Python Programming  3 \n",
      "11.  CSE       150 Web Design – Frontend Development  3 \n",
      "12.  CSE       140 Database Management Systems  3 \n",
      "Quarter -IV (Year 1)  \n",
      "13.  INT       200 Internship 1   6 \n",
      "TOTAL  42 \n",
      " \n",
      "II Year Courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 2)  \n",
      "14. CSE       210  IOT - 2 3 \n",
      "15. CSE       220  Web Programming & Scripting  3 \n",
      "16. MTH       271  Linear Algebra  3 \n",
      "17. CSE       250  Operating Systems  3 \n",
      "Quarter -II (Year 2)  \n",
      "18. CSE       270  Design and Analysis of Algorithms  3 \n",
      "19. CSE       290  Computer Networks  3 \n",
      "20. CSE       280 Advanced  Java  3 \n",
      "21. CSE       240  Data Science with R  3 \n",
      "Quarter -III (Year 2)  \n",
      "22. INT       300  Internship 2  6 \n",
      "Quarter -IV (Year 2)  \n",
      "23. CSE       230  Mobile Programming  3 \n",
      "24. MTH       221  Differential Equations  3 \n",
      "25. CSE       260  Data Visualization Tools and Techniques  3 \n",
      "26. ENG       226  English -2 3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "III Year courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 3)  \n",
      "27. INT       400  Internship 3  6 \n",
      "  Quarter -II (Year 3)   \n",
      "28. MTH       351  Advanced Statistics  3 \n",
      "29. CSE       330  Cloud Computing  3 \n",
      "30. CSE       310  Artificial Intelligence and Machine Learning  3 \n",
      "31. CSE       325  Big Data Analytics  3 \n",
      "Quarter -III (Year 3)  \n",
      "32. CSE       340  Machine Learning 1  3 \n",
      "33. CHE       305  Chemistry  3 \n",
      "34. MTH       331  Discrete Mathematics  3 \n",
      "35. CSE       360  Deep Learning  3 \n",
      "Quarter -IV (Year 3)  \n",
      "36. CSE       345  Elective I – Block Chain Technologies  3 \n",
      "37. CSE       380  Natural Language Processing  3 \n",
      "38. CSE       370  Cyber Security  3 \n",
      "39. CSE       355  Elective II - Data Science Professional \n",
      "Certification      3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "IV Year Courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 4)  \n",
      "40. CSE       440  Reinforcement Learning  3 \n",
      "41. ELE Elective III  3 \n",
      "42. CSE       410  Computer Vision  3 \n",
      "43. ELE Elective IV  3 \n",
      "Quarter -II (Year 4)  \n",
      "44. CSE       420  Software Engineering & Testing  3 \n",
      "45. ELE Elective V  3 \n",
      "46. ELE Elective VI  3 \n",
      "47. HUM       406  Humanities Course  3 \n",
      "Quarter -III & IV  (Year 4)  \n",
      "48. INT       500  Internship 4  6 \n",
      "49.  INT       510  Internship 5  6 \n",
      "TOTAL  36 \n",
      " \n",
      " \n",
      " \n",
      "TOTAL CREDITS: 162  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "B.Tech. Computer Science and Medical Engineering  \n",
      "Revised Scheme and Curriculum  \n",
      " \n",
      "I year Courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 1)  \n",
      "1.  CSE       110 Problem Solving - C Programming  3 \n",
      "2.  CSE       120 Python Programming  3 \n",
      "3.  MTH        141  Probability & Statistics   3 \n",
      "4.  ENG        126 English I  3 \n",
      "Quarter -II (Year 1)  \n",
      "5.  CSE       160 Data Structures  3 \n",
      "6.  CSE       130 IOT - 1 3 \n",
      "7.  CSE       170 Java Basics with OOPs Concepts  3 \n",
      "8.  PHY       145  Physics  3 \n",
      "Quarter -III (Year 1)  \n",
      "9.  MTH        151 Calculus  3 \n",
      "10.  CSE       180 Advanced Python Programming  3 \n",
      "11.  CSE       150 Web Design – Frontend Development  3 \n",
      "12.  CSE       140 Database Management Systems  3 \n",
      "Quarter -IV (Year 1)  \n",
      "13.  INT       200 Internship 1   6 \n",
      "TOTAL  42 \n",
      " \n",
      "II Year Courses  \n",
      "Course \n",
      "Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 2)  \n",
      "14. CSE       210  IOT - 2 3 \n",
      "15. MED       215  Introduction to Medical Science for Engineers  3 \n",
      "16. MTH       221  Differential Equations  3 \n",
      "17. CSE       220  Web Programming & Scripting  3 \n",
      "Quarter -II (Year 2)  \n",
      "18. MED       235  Introduction to Medical Devices and Systems \n",
      "for Engineers  3 \n",
      "19. CSE       240  Data Science with R  3 \n",
      "20. CSE       250  Operating Systems  3 \n",
      "21. CSE       260 Data Visualization Tools and Techniques  3 \n",
      "Quarter -III (Year 2)  \n",
      "22. INT       300  Internship 2  6 \n",
      "Quarter -IV (Year 2)  \n",
      "23. CHE       305  Chemistry  3 \n",
      "24. CSE       270  Design and Analysis of Algorithms  3 \n",
      "25. CSE       290  Computer Networks  3 \n",
      "26. ENG       226  English -2 3 \n",
      "TOTAL  42 \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "III Year courses  \n",
      " \n",
      "Course Number  Course code  Course Title  Credits  \n",
      "Quarter -I (Year 3)  \n",
      "27. INT       400  Internship 3  6 \n",
      "Quarter -II (Year 3)  \n",
      "28. CSE       310  Artificial Intelligence and Machine Learning  3 \n",
      "29. MED      315 Introduction to Medical Procedures  3 \n",
      "30.  Elective 1  3 \n",
      "31. MED     335  Medical Engineering 1  3 \n",
      "Quarter -III (Year 3)  \n",
      "32. MED       345  Medical Engineering 2  3 \n",
      "33. MED      355  Medical Engineering 3  3 \n",
      "34. MTH       351  Advanced Statistics  3 \n",
      "35. MED       365  Medical Engineering 4  3 \n",
      "Quarter -IV (Year 3)  \n",
      "36. CSE       340  Machine  Learning 1  3 \n",
      "37. CSE       360  Deep Learning  3 \n",
      "38. MTH       271  Linear Algebra  3 \n",
      "39.  Elective 2  3 \n",
      "TOTAL  42 \n",
      " \n",
      "IV Year Courses  \n",
      " \n",
      "Course \n",
      "Number  Course code  Course Title Credits  \n",
      "Quarter -I (Year 4)  \n",
      "40. CSE       410  Computer Vision  3 \n",
      "41. CSE       420  Software Engineering & Testing  3 \n",
      "42. ELE Elective 3  3 \n",
      "43. ELE Elective 4  3 \n",
      "Quarter -II (Year 4)  \n",
      "44. ELE Elective 5  3 \n",
      "45. ELE Elective 6  3 \n",
      "46. CSE       440  Reinforcement Learning  3 \n",
      "47. HUM       406  Humanities  3 \n",
      "Quarter -III & IV (Year 4)  \n",
      "48. INT       500  Internship 4  6 \n",
      "49.  INT       510  Internship 5  6 \n",
      "TOTAL  36 \n",
      " \n",
      "TOTAL CREDITS: 162\n",
      "********************************************************************************\n",
      "1. CSE 110  Problem Solving - C Programming   3 Cr \n",
      " \n",
      "This is the first  course in Problem solving with C Programming. Topics include \n",
      "understanding and applying programming logic and logic development, constants, variables, data \n",
      "types, decision making, arrays, pointers and file processing  \n",
      " \n",
      "CO1:  Understand the problem solving a spect and write algorithm.  \n",
      "CO2:  Understand and apply various concepts of C programming.  \n",
      "CO3:  Ability to work with arrays and user defined datatypes  \n",
      "CO4:  Develop and implement applications in C using functions and pointers  \n",
      "CO5:  Design applications using  file processing.  \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Logical Thinking & Programming Logic  \n",
      " Solving Logical Puzzle  \n",
      " Understanding a Program and its logic  \n",
      " Understanding a Problem and Logic development  \n",
      " Algorithm & Flow Chart  \n",
      "Module 2  \n",
      " Introduction to Programming  \n",
      " Constants, Variables &  Data Types  \n",
      " Operators & Expressions  \n",
      " Managing Input & output operations  \n",
      " Decision Making – Branching & Looping  \n",
      "Module 3  \n",
      " Storage Classes  \n",
      " Arrays  \n",
      " Structure and Union  \n",
      "Module 4  \n",
      " Pointers  \n",
      " Dynamic Memory Allocation   \n",
      " Link list  \n",
      "Module 5  \n",
      " File Processing  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "2. CSE  120 Python Programming   3 Cr \n",
      " \n",
      "An introductory course in Python Programming. The course is approached as the foundation \n",
      "for the courses to follow.  Topics include Basic Syntax, Variable and data types, Operator, conditional \n",
      "and control statements, string manipulations, functions, modules, attributes, match and search \n",
      "functions and applications.  \n",
      " \n",
      "CO1:  Explore various operators and Implement Conditionals and Loops for Python Programs.  \n",
      "CO2:  Represent compound data using Python lists, tuples, dictionaries and  string.  \n",
      "CO3:  Implement and analyze the concept of modular programming and error handling in python.  \n",
      "CO4:  Design the program models using Object -Oriented Programming concepts such as encapsulation, \n",
      "inheritance and polymorphism as used in Python.  \n",
      "CO5:  Identify and implement the various string manipulate operations using regular expressions.  \n",
      " \n",
      "Module 1  \n",
      " Basic Syntax, Variable and data types, Operator  \n",
      " Conditional Statements  \n",
      " Looping  \n",
      " Control Statements  \n",
      "Module 2  \n",
      " String Manipulation  \n",
      " Lists  \n",
      " Tuple  \n",
      " Dictionaries  \n",
      "Module 3  \n",
      " Functions  \n",
      " Modules  \n",
      " Input/output  \n",
      " Exception Handling  \n",
      "Module 4:  \n",
      " Class and object   \n",
      " Attributes  \n",
      " Inheritance   \n",
      " Overloading  \n",
      " Overriding   \n",
      " Data hiding   \n",
      "Module 5  \n",
      " Regular expressions  \n",
      " Match function   \n",
      " Search function   \n",
      " Matching VS Searching  \n",
      " Modifiers  \n",
      " Patterns  \n",
      " \n",
      "********************************************************************************\n",
      "3. CSE 130  IoT - 1  3 Cr  \n",
      " \n",
      "This is the first course in Internet of Things. It starts with an introduction to basic electronics \n",
      "and electronic components, integrated circuits and boards, Arduino micro controllers and the \n",
      "application of microcontrollers to build  internet of things.  \n",
      " \n",
      "CO1:  Understand the basics of Electrical circuits, loads and breadboard connection  \n",
      "CO2:  Model a circuit and analyze its operation based on the concepts learned from various types of \n",
      "material, circuit elements & electronic devices  \n",
      "CO3:  Ability to understand and develop simple projects using the concept of Integrated circuits, logic \n",
      "gates and ARDUINO board architecture  \n",
      "CO4:  Ability to develop project by connecting and controlling various electronic devices by using \n",
      "Arduino platform and programming concepts  \n",
      "CO5:  Ability to build an IoT project by understanding sensors and its automated applications, motor \n",
      "control and communication technologies  \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Introduction to Electrical Sources  \n",
      " Battery, types and its ratings  \n",
      " Switch and Types  \n",
      " Introduction to Electrical and Electronic loads  \n",
      " Low power loads: LED, Buzzer, Speaker DC Motor, Gear Motor, Servomotor, Display  \n",
      " Breadboard basics  \n",
      " \n",
      "Module 2  \n",
      " Introduction to conductors, Insulators, semiconductors  \n",
      " Circuit elements: Resistors, Inductors, Capacitors  \n",
      " Devices and its operation: Diode, LED, Photodiode, LDR, Transistor: BJT, PNP, NPN, Photo \n",
      "transistor, IR Receiver, Thermistor  \n",
      " \n",
      "Module 3  \n",
      " Integrated Circuits Pin diagram and applications: IC741 -  555 Timer -  LM358, LM386, \n",
      "LM353 - Counter IC  \n",
      " Logic gates: AND, OR, NAND, NOR, NOT, EX -OR, EX -NOR  \n",
      " Introduction to IoT and its applications  \n",
      " ARDUINO: Board architecture  \n",
      " \n",
      "Module 4  \n",
      " Programming with ARDUINO UNO board  \n",
      " Working with LEDs  \n",
      " Operating LED with time delay  \n",
      " Working with digital  switch  \n",
      " Adjusting voltage using potentiometer  \n",
      " Introduction to sensor Integration with ARDUINO  \n",
      " \n",
      "********************************************************************************\n",
      "Module 5  \n",
      " Sensor and its automated applications using ARDUINO: LDR:  illumination control, \n",
      "Ultrasonic sensor, DHT 11 sensor, PIR Sensor  \n",
      " Working with Servo motor  \n",
      " Establishment of communication using Bluetooth  \n",
      "  \n",
      "********************************************************************************\n",
      "4. ENG  126  English -1  3 Cr  \n",
      " \n",
      "As the first course in English, it focuses on reading, vocabulary building, grammar, \n",
      "comprehension, speaking, writing, and communicating.  Confidence building and technical \n",
      "communication are considered the essential outcomes of the course.  \n",
      " \n",
      "CO1:  Understanding of the Process of Communication  \n",
      "CO2:  Improved Listening comprehension of English language and ability to clear IELTS Listening \n",
      "module  \n",
      "CO3:  Improved Speaking skills of English language and ability to clear IELTS Speaking module  \n",
      "CO4:  Improved Reading comprehension of English language and ability to clear IELTS Reading \n",
      "module  \n",
      "CO5:  Improved Writing skills of English language and ability to clear IELTS Writing module  \n",
      " \n",
      " \n",
      "Module 1 – COMMUNICATIVE ENGLISH: AN INTRODUCTION  \n",
      "Communication: definition, types - Process of Communication - Channels of Communication - Barriers \n",
      "of Communication - Paralinguistic features - Communicative English: Definition, Purpose – LSRW - \n",
      "General Awarenes s on Language Proficiency Tests: IELTS, TOEFL, PTE  \n",
      " \n",
      "Module 2 - LISTENING  \n",
      "Active Listening and Passive Listening – Barriers to Listening – Listening Exercises: conversations \n",
      "between two people set in an everyday social context, monologues set in an everyday social context, , \n",
      "conversations between up to four people set in an educational or training context,  monologues on an \n",
      "academic subject etc.  \n",
      " \n",
      "Module 3 – SPEAKING  \n",
      "Basic Phonetics - Pronunciation - Fluency and Coherence - Syllables -Public Speaking -Attending a n \n",
      "Interview,  Casual Conversations, Group Discussion.  \n",
      " \n",
      "Module 4 – reading  \n",
      "Effective reading -Types of Reading – Reading Comprehension:  multiple choice, identifying \n",
      "information, identifying the writer’s views/claims, matching information, matching headings , \n",
      "matching features, matching sentence endings, sentence completion, summary completion, note \n",
      "completion, table completion, flow -chart completion, diagram label completion and short -answer \n",
      "questions  \n",
      " \n",
      "Module 5 – WRITING  \n",
      " Formal Letter: Introduction, Types, Format and Structure – Reports: Academic and Business – Short \n",
      "Story Writing: Eight -point Arc of Short Story Writing, Writing dialogues for short stories, Story \n",
      "Writing based on Pictures, Story Writing using an Outline –Resume and Cover Letter:  Difference \n",
      "between CV, Resume and Biodata, Resume Writing, Cover Letter Writing – Writing a Movie Review: \n",
      "Structure and Vocabulary – Explaining Flow Chart/Bar Diagram: Structure and Vocabulary – Essay \n",
      "Writing  \n",
      " \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "5. CSE 160  Data Structures   3 Cr \n",
      " \n",
      "This is the first course in data structures. It deals with arrays, pointers, use of arrays, \n",
      "searching, sorting  and analysis of algorithms,  \n",
      " \n",
      "CO1  To impart a thorough understanding and application of dynamic data structure.   \n",
      "CO2  To design and implement the linear data structures like stack and queue using dynamic \n",
      "implementation.   CO3  To represent and manipulate data using nonlinear data structures like trees and graphs to design \n",
      "algorithms for various applications.   \n",
      "CO4  To understand and apply elementary algorithms: sorting, searching and hashing.         \n",
      "CO5  To analyze the efficiency of programs based on time and space complexity.                 \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Arrays & Vectors  \n",
      " Static and Dynamic Arrays  \n",
      " Linked List, Circular List  \n",
      " Pointers  \n",
      " Implentation of list using pointers  \n",
      "Module 2  \n",
      " Stack  \n",
      " Queue  \n",
      " Implentation using Arrays and Linked List  \n",
      "Module 3  \n",
      " Tree \n",
      " Binary Tree  \n",
      " BST \n",
      " Graph  \n",
      " Traversal algorithm  \n",
      "Module 4  \n",
      " Searching Algorithms  \n",
      " Sorting Algorithms  \n",
      " Hashing  \n",
      "Module 5  \n",
      " Analysis of Algorithm  \n",
      " Efficiency Calculation  \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "6. MTH 141  Probability and Statistics   3 Cr  \n",
      " \n",
      "This is the first course in Probability and Statistics. Topics include Variable and its Types, \n",
      "Sampling, Experimental Design, Histograms, Clusters, Time Series Graph, Randomness, Simulation, \n",
      "Anaova, T -Test, and Distributions  \n",
      " \n",
      "CO1:  To learn the basics of st atistics and sampling techniques  \n",
      "CO2:  To study about frequency distributions and related graphs  \n",
      "CO3:  To acquire the knowledge of measures of central tendency and variation  \n",
      "CO4:  To obtain the basic ideas of probability theory through random experiments  \n",
      "CO5:  To learn the beginner level concepts of inferential statistics  \n",
      " \n",
      "Module 1  \n",
      " Introduction to statistics  \n",
      " Individuals and variables  \n",
      " Population  \n",
      " Parameter  \n",
      " Types of variables: Quantitative, Categorical  \n",
      " Sampling: Random, Stratified, Systematic, Cluster and Errors  \n",
      " Experimental design  \n",
      " Avoiding bias in survey design  \n",
      " Randomization: Placebo effect, blocked randomization, Blinding  \n",
      "Module 2  \n",
      " Frequency histograms  \n",
      " Frequency table  \n",
      " Relative Frequency table  \n",
      " Stem and leaf plot  \n",
      " Distribution  \n",
      " Clusters  \n",
      " Peaks  \n",
      " gaps and Outliers  \n",
      " Time series graph, bar graph, Pareto chart, and pie chart  \n",
      "Module 3  \n",
      " Measures of central tendency: Mode, Median, Mean, Trimmed mean  \n",
      " Weighted average, Central tendencies and distributions  \n",
      " Measures of var iation: Standard deviation, Variance  \n",
      " Coefficient of variation  \n",
      " Chebyshev’s Theorem  \n",
      " Percentiles, IQR, Box and Whisker plots  \n",
      " Scatter diagrams  \n",
      "Module 4  \n",
      " Basic theoretical probability  \n",
      " Probability using sample spaces  \n",
      " Probability of basic set operations  \n",
      "********************************************************************************\n",
      " Experiment al probability, randomness, probability and simulation  \n",
      " Addition and multiplication rule for probability, conditional probability and independence  \n",
      "Module 5  \n",
      " Linear correlation, Linear regression, Least square criterion  \n",
      " Coefficient of determination  \n",
      " T-test \n",
      " Anova \n",
      " Normal distributions and empirical rule  \n",
      " Z-scores and Probabilities, Using Z -tables  \n",
      " Sampling distributions and the central limit theorem  \n",
      "  \n",
      "********************************************************************************\n",
      "7. CSE 170  Java Basics with OOPs  Concepts   3 Cr  \n",
      " \n",
      "Java Programming is taught as an essential building block of Computer science and \n",
      "engineering program. It presents the basics of object oriented programming, and its applications.  \n",
      " \n",
      "CO1:  Design and implement basic data types, array and control flow  using J2SE  \n",
      "CO2:  Demonstrate constructors and method in java  \n",
      "CO3:  Implement Java Program using object -oriented concepts Data Abstraction, Polymorphism  \n",
      "CO4  Apply the concepts of java packages, inheritance, and Exception handling mechanisms for real -\n",
      "world problems.  \n",
      "CO5  Demonstrate multitasking using Threads and Develop simple applications using GUIs and event -\n",
      "driven  \n",
      "Programming  \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Object Oriented Programming Basics  \n",
      " Class and Objects  \n",
      " Variables and data types  \n",
      " Conditional and looping constructs  \n",
      " Arrays  \n",
      "Module 2  \n",
      " Fields and Methods  \n",
      " Constructors  \n",
      " Overloading methods  \n",
      " Garbage collection  \n",
      " Nested classes  \n",
      "Module 3  \n",
      " Inheritance  \n",
      " Overriding methods  \n",
      " Polymorphism  \n",
      " Making methods and classes final  \n",
      "Module 4  \n",
      " Abstract classes and methods  \n",
      " Interfaces  \n",
      " String and String Conversions  \n",
      " Exception Handling  \n",
      "Module 5  \n",
      " Packages  \n",
      " Applets  \n",
      " Threads  \n",
      " AWT & Swings Basics  \n",
      " Database Connectivity  \n",
      " \n",
      "********************************************************************************\n",
      "8. CSE 140          Database Management Systems   3 Cr  \n",
      " \n",
      " As the first course in database management system, it  considers database types, naming \n",
      "conventions, look up tables, updating  and deleting data, database design and database  schema.  \n",
      " \n",
      "CO1:  Explore the various methods to develop relational databases and their  types  \n",
      "CO2:  Design and implement ER diagram & database schema for a given problem and apply \n",
      "normalization for the given database application.  \n",
      "CO3:  Build database using Data Definition Language Statements and perform basic CRUD \n",
      "operations using Data Manipulation Language statements like Insert, Update and Delete.  \n",
      "CO4:  To know about joins and types of joins, Also learn how to join the tables and about \n",
      "Subqueries, types of operators like union.  \n",
      "CO5:  To expose students to design and develop a database for real -time applications.  \n",
      "  \n",
      " \n",
      "Module 1  \n",
      " Relational databases and Codd rules  \n",
      " Tools You Need f or Database Design  \n",
      " MySQL Workbench  \n",
      " Database and Types  \n",
      " \n",
      "Module 2  \n",
      " Identifying Entities  \n",
      " Defining the Attributes  \n",
      " Normalization  \n",
      " Data Types and Precision  \n",
      " Integrity Constraints  \n",
      " Naming Conventions  \n",
      " Lookup Tables and Auditing  \n",
      " \n",
      "Module 3  \n",
      " Command Line & Inserting Data  \n",
      " Basic Select Queries  \n",
      " Updating & Deleting Data  \n",
      " Aliases & Joins  \n",
      " \n",
      "Module 4  \n",
      " Union, Concat & Count  \n",
      " Using The IN Clause  \n",
      " Math & Subqueries  \n",
      " Using Group By  \n",
      " \n",
      "Module 5  \n",
      " DB Design  \n",
      " Finalize Database Model  \n",
      " Generate the Physical Database Schema  \n",
      "********************************************************************************\n",
      "9.  MTH 151  Calculus   3 Cr  \n",
      " \n",
      "The first course in Calculus dealing with functions, limits, derivatives, application of \n",
      "differentiation, integral calculus and applicati on of integration.  \n",
      " \n",
      "CO1:  To learn the basic information about functions with limits and continuity  \n",
      "CO2:  To understand the ideas of derivatives, higher order derivative and associated rules  \n",
      "CO3:  To describe the concepts and applications of derivatives and  higher order derivatives  \n",
      "CO4:  To acquire the basic ideas of integration  \n",
      "CO5:  To apply the techniques of integral to various problems of finding length of plane curves and  \n",
      "area between the curves  \n",
      " \n",
      "Module 1 : Functions   \n",
      " Representing functions  \n",
      " Mathematical models  \n",
      " Generating new functions  \n",
      " \n",
      "Module 2: Limits  \n",
      " Rates of change, velocity, and tangents  \n",
      " The limit of a function   \n",
      " Limit laws  \n",
      " d.Continuity  \n",
      " \n",
      "Module 3 Derivatives  \n",
      " Derivatives and rates of change  \n",
      " The derivative as a function  \n",
      " Differentiation f ormulae  \n",
      " Derivatives of trigonometric functions  \n",
      " The chain rule  \n",
      " Implicit Differentiation  \n",
      " Related Rates  \n",
      " Linear Approximations and Differentials  \n",
      " Applications to other fields  \n",
      " \n",
      "Module 4: Applications of Differentiation   \n",
      " Maximum and minimum values  \n",
      " The mean value  theorem  \n",
      " Curve sketching  \n",
      " Optimization  \n",
      " Newton's method  \n",
      " Antiderivatives  \n",
      " \n",
      "Module 5: Integration   \n",
      " Area and distances  \n",
      " The definite integral  \n",
      " The Fundamental Theorem of Calculus  \n",
      "********************************************************************************\n",
      " Indefinite Integrals  \n",
      " Substitution  \n",
      " \n",
      "Module 6 Application of Integration  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "10. CSE  180 Advanced Python Programming   3 Cr  \n",
      " \n",
      "Second course in Python Programming. The course is approached as the advanced course \n",
      "based on the prior course on basics in python programming. Topics include Python Distributions, \n",
      "Data Sciences and Tools, Data Prepa ration, Data Cleansing, Data Normalization, Data \n",
      "Standardization, Linear Regression,  Logistic regression, Support Vector Machine, Working with Iris \n",
      "dataset, Accuracy Prediction,  K-Mean and K -nearest.  \n",
      " \n",
      "CO1:  Introducing Python IDE’s and Implement Basic Data  Science Concepts, Data Preparation, \n",
      "Data Cleansing, Data Normalization, Data Standardization  \n",
      "CO2:  Analyse and Incorporate the various options using Numpy and Pandas to analyse the data  \n",
      "CO3:  Understand the importance of data visualization and the desig n and use of many visual \n",
      "components  \n",
      "CO4:  Demonstrate the various machine learning techniques using the Sklearn package and apply \n",
      "the concept of unsupervised learning and Clustering for applications and calculate the \n",
      "accuracy score . \n",
      "CO5:  \n",
      " Demonstrate Tensor  Flow Data Structures, Implement Data Capstone Projects with Advance \n",
      "Python Concepts and Machine Learning Capabilities  \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Python Distributions  \n",
      " Data Sciences and Tools  \n",
      " Jupyter Notebook  \n",
      " Spyder  \n",
      " Data Preparation  \n",
      " Data Cleansing  \n",
      " Data Normalization  \n",
      " Data Standardization  \n",
      "Module 2  \n",
      " Python Data Analysis – Numpy  \n",
      " Array  \n",
      " Indexing  \n",
      " Operations  \n",
      " Pandas  \n",
      " Operations  \n",
      " GroupBy  \n",
      " Quantitative and Qualitative Data Analysis  \n",
      " Pandas Data Structures  \n",
      "Module 3  \n",
      " Data Visualization with matplotlib  \n",
      " Plotting Curve s  \n",
      " Scatter Plot  \n",
      " Bar Chart and Histogram  \n",
      " Box plot  \n",
      "********************************************************************************\n",
      " Pie-chart  \n",
      " Data Visualization with Seaborn  \n",
      " Built -in Data Visualization  \n",
      " Plotly and Cufflinks  \n",
      "Module 4  \n",
      " Linear Regression  \n",
      " Logistic regression  \n",
      " Support Vector Machine  \n",
      " Working with Iris dataset  \n",
      " Accuracy Prediction  \n",
      " K-Mean and K -nearest   \n",
      "Module 5  \n",
      " Tensorflow  \n",
      " Data Capstone projects  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "11. CSE 150  Web Design - Frontend Development   3 Cr  \n",
      " \n",
      "The course focuses on the basics of Web design and application.  Starting with the structure of HTML \n",
      "document, it leads  to web design and development.  \n",
      " \n",
      "CO1:  Ability to create responsive web pages with multimedia content using HTML 5  \n",
      "CO2:  Acquire skills in developing dynamic web pages using CSS.  \n",
      "CO3:  Ability to create interactive webpages using Typescript and enhance its functionality using Angular \n",
      "and its services.  \n",
      "CO4:  Ability to develop multipurpose and customizable webpages using Bootstrap framework.  \n",
      "CO5:  Design a responsive webpage with Authentication and Authorization.  \n",
      " \n",
      "Module 1  \n",
      " Structure of HTML document  \n",
      " HTML Basics  \n",
      " Form and Form inputs  \n",
      " Tables and Lists  \n",
      " Multimedia Tags  \n",
      " HTML5 Graphics  \n",
      " \n",
      "Module 2  \n",
      " Introduction to CSS  \n",
      " Introduction to Bootstrap  \n",
      " Grid Systems  \n",
      " Tables  \n",
      " Alerts, Button and Button groups  \n",
      " Inputs  \n",
      " Accordian and Tab Menu  \n",
      " Glyphicons  \n",
      " Tooltip  \n",
      " Model Pop ups  \n",
      " \n",
      "Module 3  \n",
      " Introduction to Angular Basics  \n",
      " First Angular APP  \n",
      " Introduction to Typescript  \n",
      " Building Blocks of Angular  \n",
      " Components  \n",
      " Directives  \n",
      " Services  \n",
      " Dependency Injection  \n",
      " \n",
      "Module 4  \n",
      " Bootstrap integration  \n",
      " Bindings  \n",
      " Building reusable components  \n",
      "********************************************************************************\n",
      " Custom Directives  \n",
      " Consuming Http Services  \n",
      " Routing and Navigations  \n",
      " \n",
      "Module 5  \n",
      " Authentication and Authorization  \n",
      " Design Project  \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "12. PHY 145  Physics   3 Cr  \n",
      "This is an introductory engineering physics course. It deals with electricity, magnetism, electronics, \n",
      "laws of mechanics and thermodynamics.  \n",
      "CO1: To understand the concepts of Electric fields, Potentials, and Electric circuits with practical \n",
      "knowledge  CO2: To acquire  the knowledge on concept of Magnetic forces, Fields, Capacitance and \n",
      "Electromagnetic induction  CO3: To understand the characteristic of semi -conductors and its application through basic theories with \n",
      "Practical knowledge  CO4: To know the application of Mechanical laws, and forces.  \n",
      "CO5:  To gain the concepts of Thermodynamics and fluids with Practical applications  \n",
      "Module 1 : Electric  Forces and Fields  \n",
      "Electric Forces and Fields  \n",
      "Electric Potential  \n",
      "Gauss's Law  \n",
      "Electric Work and Energy  \n",
      "Electric Current and Circuits  \n",
      " \n",
      "Module 2   Magnetic Forces and Fields  \n",
      " \n",
      "Magnetic Forces and Fields  \n",
      "Capacitors and Dielectrics  \n",
      "Magnetic Interactions and Field  \n",
      " \n",
      "Module 3  Electric Circuits and Electronics  \n",
      "Module 4  Mechanics  \n",
      "Motion and Vectors  \n",
      "Forces a nd Newton's Laws  \n",
      "Circular and Rotational Motion  \n",
      "Equilibrium and Elasticity  \n",
      "Momentum and Collisions  \n",
      "Energy and Work  \n",
      " \n",
      "Module 5: Thermodynamics and Fluids  \n",
      " \n",
      "Thermodynamics  \n",
      "Fluids  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "13. INT 200  INTERNSHIP    6 Cr \n",
      " \n",
      "CO1  Use knowledge and skills acquired in science and engineering to explore and critically \n",
      "analyse a real -world problem; use creativity and innovation to identify and critically \n",
      "evaluate potential solutions to the problem; choose and implement a particular \n",
      "solution ( Problem statement, Methodology, Innovation, Project comprehension)  \n",
      "CO2  Design, implement and evaluate a software system to meet desired needs within \n",
      "relevant constraints; evaluate and analyse the outcomes and impact of the system                 \n",
      "( Design, Implementation & Broader impact)  \n",
      "CO3  Use of modern techniques and tools, communication tools, standard documentation \n",
      "and testing practices (Usage of modern tool/technology)  \n",
      "CO4  Conduct experiments to test software systems and interpret results to debug and \n",
      "improve system; achieve the proposed solution  (Real time Testing & \n",
      "Result/Outcome)  \n",
      "CO5  Work effectively toward a common objective in software development teams and \n",
      "contribute effectively; Apply verbal and written communication skills to expla in \n",
      "technical problem solving techniques and solutions to global audience (Each \n",
      "Students' contribution, Communication , Presentation  content)  \n",
      "CO6  Exhibit independent learning and professional development and practice life -long \n",
      "learning (Learning from reso urces & Future scope)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "14.  CSE  210   IoT- 2      3 Cr  \n",
      " \n",
      "This is the second course in Internet of Things. It builds on the concepts learned in Python \n",
      "Programming and the Use of Arduino Micro controllers in the first course. The course focuses on \n",
      "Rasberry Pi as the hardware tool to design and build IOTs. Python i s a mandatory background to \n",
      "program, control and network the system built in IOT -2.  \n",
      " \n",
      "CO1:  Understand the basic concepts of IoT and get familiar with the features of Raspberry Pi  \n",
      "CO2:  Experiment the network setup and configurations in headless mode of operation by learning \n",
      "various remote addressing methods  \n",
      "CO3:  Learn OS basics, code and interpret Linux terminal commands for Raspberry Pi  \n",
      "CO4:  Develop project using Python -Linux terminal command code for controlling the components \n",
      "connected to Raspberry Pi and to realize the application of Raspberry Pi to access SMTP \n",
      "inbox, to install webserver and to manipulate GPIO pins  \n",
      "CO5:  Build home automation projects using Raspberry Pi by interfacing electronic components and \n",
      "to learn the blog hosting with Raspber ry Pi  \n",
      " \n",
      " \n",
      "Module 1  \n",
      " Introduction to Raspberry Pi  \n",
      " Hardware Description & Pin Configuration  \n",
      " Raspberry Pi B+  \n",
      "Preparing SD card for installation and configuration  \n",
      " \n",
      "Module 2  \n",
      " Network Setup  \n",
      " GPIO Setup  \n",
      " Pi Using Secure Shell (SSH)  \n",
      " Pi over VNC  \n",
      "Module 3  \n",
      " OS Basics  \n",
      " Linux  Basics  \n",
      " Linux Commands  \n",
      " File Structure and Permissions  \n",
      "  \n",
      "Module 4  \n",
      " Python for Raspberry Pi  \n",
      " Accessing SMTP inbox using Python  \n",
      " Manipulating GPIO pins using Python  \n",
      " Making Raspberry Pi as a Web Server  \n",
      "Module 5  \n",
      " Hosting a blog on Raspberry Pi  \n",
      " Interfacing Electronic  Component on Raspberry – Pi \n",
      " Home Automation using Raspberry - Pi \n",
      " \n",
      "********************************************************************************\n",
      "15.  CSE 220  Web Programming and Scripting  3 Cr  \n",
      " \n",
      "CO1:  Create effective scripts using JavaScript to enhance the end user experience.  \n",
      "CO2:  Describe and utilize JavaScript (ES6) advanced programming concepts such as Arrow \n",
      "functions, Template literals, etc.  \n",
      "CO3:  Demonstrate Knowledge of Node Package Manager and Mongo DB concepts.  \n",
      "CO4:  Implement web applications using Express JS framework.  \n",
      "CO5:  Test, debug, and deploy JavaScript -based web  applications.  \n",
      " \n",
      " \n",
      "Module 1 Introduction – Java Script (JS)  \n",
      " \n",
      " Software Setup  \n",
      " JS - Introduction  \n",
      " JS - Types and Variables  \n",
      " JS - Objects  \n",
      " JS - Functions  \n",
      " JS - Operators  \n",
      " JS - Expressions vs Statements  \n",
      " JS - Scopes  \n",
      " JS - Loops and Conditional Statements  \n",
      " \n",
      "Module 2 JS - Advanced Topics  \n",
      " \n",
      " Variables Lifecycles  \n",
      " ES6 Arrow functions  \n",
      " ES5.1 Array Helper Methods  \n",
      " ES6 Template Literals  \n",
      " ES6 Rest/Spread Operators and Default Function Parameters  \n",
      " ES6 Enhanced Object Literals  \n",
      " ES6 Array and Object Destructuring  \n",
      " ES6 Classes, Prototypes and Function Constructors  \n",
      " Babel Introduction  \n",
      " NPM - Node Package Manager  \n",
      " Webpack  \n",
      " Introduction to the MongoDB  \n",
      " \n",
      "Module 3 Express JS  \n",
      " \n",
      " Introduction to JS  \n",
      " Routing  \n",
      " Template Engines  \n",
      " Middleware  \n",
      " Web App Components  \n",
      " Integrating a Database  \n",
      "********************************************************************************\n",
      " Error Handling, Debugging, Se curity, and Optimization  \n",
      " Proper App Structure  \n",
      " Creating an Embedded Map App  \n",
      " Creating a Chat App  \n",
      " \n",
      "References  \n",
      " \n",
      "https://www.udemy.com/course/javascript -bible/  \n",
      "https://www.udemy.com/course/expressjs -from -beginner -to-advanced/  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "16.  MTH 271   Linear Algebra   3 Cr  \n",
      "CO1  To analyze the solution set of system of linear equations using matrices  \n",
      "CO2  \n",
      " To generalize the concepts of a real (complex) vector space to an arbitrary finite -dimensional \n",
      "vector space  \n",
      "CO3  To investigate properties of vector spaces and subspaces using by linear transformations  \n",
      "CO4  To acquire knowledge of matrix diagonalization using eigen values and eigenvectors  \n",
      "CO5  To study about scalar product and normalization of vectors using inner products  \n",
      "Module 1  \n",
      "Fields, Systems of Linear Equations, Matrices and Elementary Row Operations, Row -Reduced \n",
      "Echelon Matrices, Matrix Multiplication, Invertible Matrices  \n",
      "Module 2  \n",
      "Vector spaces (VS), subspaces, linear independence, span, column space, row space, null space, basis, \n",
      "dimension.  \n",
      "Module 3  \n",
      "Linear transformations, matrix of a linear transform ation, null spaces and ranges, Invertibility, rank-\n",
      "nullity theorem and its applications, change of basis, similarity, determinants, transpose.  \n",
      "Module 4  \n",
      "Eigenvalues, eigenvectors, characteristic polynomials, minimal polynomials, Cayley -Hamilton \n",
      "theorem, algebraic multiplicity, geometric multiplicity, diagonalization.  \n",
      " \n",
      "Module 5  \n",
      "Inner products, inner product spaces, linear functionals and adjoints, unitary and normal operators, \n",
      "Gram -Schmidt process. Projections, least squares for model -fitting and applications.  \n",
      " \n",
      "References:  \n",
      " Sheldon Axler , Linear algebra done right, Springer publications.  \n",
      " David C. Lay, Steven R Lay and Judi J. McDonald , Linear Algebra and Its \n",
      "Applications , Fifth edition,  Pearson education limited.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "17.  CSE 250    Operating Systems   3 Cr  \n",
      " \n",
      "CO1:  To impart a thorough understanding and the evolution of the operating system and how the \n",
      "operating system manages the hardware devices and coordinates them.  \n",
      "CO2:  To understand and gain knowledge on CPU: Process scheduling and synchronization  \n",
      "CO3:  To analyze the various memory hierarchy and able to calculate the access times  \n",
      "CO4:  To understand the various concepts involved in file system management  \n",
      "CO5:  To implement and understand OS concepts through a few case studies  \n",
      " \n",
      " \n",
      "Module 1 : Fundamentals of OS  \n",
      " Overview of operating systems functionalities  \n",
      " Hardware concepts related to OS  \n",
      " CPU states  \n",
      " I/O channels  \n",
      " Memory Hierarchy  \n",
      " Microprogramming  \n",
      "Module 2: Process  \n",
      " The concept of a process  \n",
      " Job and processor scheduling  \n",
      " Scheduling Algorithms  \n",
      " Synchronisation & Deadlock  \n",
      " Inter process Communication  \n",
      "Module 3: Memory Management   \n",
      " Memory Organisation and Management  \n",
      " Storage Allocation  \n",
      " Virtual memory concep ts  \n",
      " Paging and segmentation  \n",
      " Address mapping  \n",
      " Virtual Storage Management  \n",
      "Module 4: File Organisation  \n",
      " Page replacement strategies.  \n",
      " File organisation  \n",
      " File and Directory structures  \n",
      " File Structure  \n",
      "Module 5 :Case Study  \n",
      " Case Study on OS  \n",
      " \n",
      "********************************************************************************\n",
      "References:  \n",
      "http://web.stanford.edu/~ouster/cgi -bin/cs140 -spring19/index.php  \n",
      "http://web.stanford.edu/~ous ter/cgi -bin/cs140 -spring19/projects.php  \n",
      "https://swayam.gov.in/nd1_noc20_cs04/preview  \n",
      "http://www.cs.tufts.edu/comp/111/  \n",
      "https://www.cs.cmu.edu/~410/expectations.html https://www.cs.cmu.edu/~410/projects.html  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "18.  CSE  270          DESIGN AND ANALYSIS OF ALGORITHMS  3 Cr  \n",
      " \n",
      "CO1:  \n",
      " Ability to analyze the asymptotic performance of algorithms and understand divide and \n",
      "conquer design techniques.  \n",
      "CO2:  Design of algorithmic techniques like backtracking, dynamic programming, greedy, branch \n",
      "& bound, and performance analysis of online, offl ine and randomized algorithms  \n",
      "CO3:  Design of algorithms for network security applications.  \n",
      "CO4:  Design of algorithms for geometric modelling and computer graphic applications.  \n",
      "CO5:  Design of algorithms for parallel and distributed computing based applications.  \n",
      " \n",
      " \n",
      "Module 1: Introduction:  \n",
      "Role of Algorithms in computing, Analysis of Algorithms, Asymptotic notation, Euclid's  algorithm, \n",
      "Problem, Instance, RAM model, Principles of Algorithm Design, Sorting Algorithm - Insertion Sort & \n",
      "Complexity Analysis, Divide and Conquer Technique, Solving recurrences - substitution, Iteration, \n",
      "Recursion tree, Changing variable and Master's Method.  \n",
      " \n",
      "Module 2: Combinatorial Optimization:  \n",
      "Backtracking; Dynamic programming; Greedy Technique; Branch & Bound  \n",
      " \n",
      "Modul e 3: Advanced Algorithmic Analysis:  \n",
      "Amortized analysis; Online and offline algorithms; Randomized algorithms,  \n",
      "NP Completeness  \n",
      " \n",
      "Module 4: Cryptographic Algorithms:  \n",
      "Historical overview of cryptography; Private -key cryptography and the key -exchange  problem;  \n",
      "Public -key cryptography; Digital signatures; Security protocols; Applications (zero knowle dge proofs, \n",
      "authentication etc.,  \n",
      " \n",
      "Module 5: Geometric Algorithms:  \n",
      "Line segments: properties, intersections; convex hull finding algorithms, Voronoi Diagram,  Delaunay \n",
      "Triangulation  \n",
      " \n",
      "Module 6 Parallel Algorithms:  \n",
      "PRAM model; Exclusive versus concurrent reads and writes; Pointer jumping; Brent’s  theorem and \n",
      "work efficiency  \n",
      " \n",
      "Module 7   Distributed Algorithms:  \n",
      "Consensus and election; Termination detection; Fault tolerance; Stabilization  \n",
      " \n",
      " \n",
      "References:  \n",
      "https://online.stanford.edu/courses/cs161 -design -and-analysis -algorithms  \n",
      "https://www.cse.iitm.ac.in/course_details.php?arg=OTI=  \n",
      "https://www.cs.ox.ac.uk/teaching/courses/2018 -2019/alg design/  \n",
      "https://www.athabascau.ca/syllabi/comp/comp372.php  \n",
      " \n",
      "********************************************************************************\n",
      "19.  CSE   290   Computer Networks   3 Cr  \n",
      " \n",
      "CO1:  To visualize the different aspects of networks, protocols and network design models.  \n",
      "CO2:  To examine various Data Link layer design issues and Data Link protocols.  \n",
      "CO3:  To analyze and compare different LAN protocols.  \n",
      "CO4:  To compare and select appropriate routing algorithms for a network, outline the mechanisms  \n",
      "involved in transport layer.  \n",
      "CO5:  To examine the important aspects and functions of network layer, transport layer and \n",
      "application layer in internetworking.  \n",
      " \n",
      " \n",
      "Module 1: Basics of Networks  \n",
      " Topologies  \n",
      " Layering and protocols  \n",
      " Internet Architecture  \n",
      " Network Tools  \n",
      "Module 2 : Data Link Layer  \n",
      " Link layer Services  \n",
      " Framing  \n",
      " Error Detection  \n",
      " Flow control  \n",
      " Media access control  \n",
      " Ethernet (802.3)  \n",
      " Wireless LANs – 802.11  \n",
      " Bluetooth  \n",
      "Module 3 : Network Layer  \n",
      " IP Addressing  \n",
      " Switching and bridging  \n",
      " Internetworking  \n",
      " Routing  \n",
      " Multicast – addresses – multicast routing  \n",
      "Module 4 : Transport Layer  \n",
      " UDP / TCP  \n",
      " Connection management  \n",
      " TCP Congestion control  \n",
      " QoS  \n",
      "Module 5: Application Layer  \n",
      " Electronic Mail  \n",
      " HTTP  Web Services  \n",
      " DNS  \n",
      " SNMP  \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "References:  \n",
      "https://www.cse.wustl.edu/~jain/cis677 -96/toc.html  \n",
      "https://www.adelaide.edu.au/course -outlines/002328/1/sem -1/ \n",
      "https://www.tru.ca/distance/courses/comp3271.html  \n",
      "https://swayam.gov. in/nd2_ugc19_cs10/preview  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "********************************************************************************\n",
      "20.  CSE 2 80  Advanced Java  3 Cr  \n",
      " \n",
      "CO1  Explore the various methods to develop dynamic web applications using JSP.  \n",
      "CO2  Implement the request -response interaction between client and server using servlets.  \n",
      "CO3  Implement and analyze the Java Application Programming Interface to develop \n",
      "communication between the client and the database..  \n",
      "CO4:  Implement Plain Old Java Object (POJO) model using spring framework  \n",
      "CO5:  Perform Java Object – Database mapping techniques using Hibernate.  \n",
      " \n",
      "Module 1: JAVA Server Page (JSP)  \n",
      " JSP – Introduction, Life cycle of JSP and Directory structure of JSP  \n",
      " Implicit object in JSP  \n",
      " JSP Scripting Elements  \n",
      "o JSP Expressions  \n",
      "o JSP Scriptlets  \n",
      "o JSP Declarations  \n",
      " CRUD in JSP (Create, Read,  Update and Delete)  \n",
      " Reading HTML form data with JSP (GET, POST, PUT, DELETE)  \n",
      " \n",
      "Module 2: Servlets  \n",
      " Servlet fundamentals – Life Cycle  \n",
      " Servlet form data - Reading Servlet parameters  \n",
      " Servlet client request and server response  \n",
      " Handling cookies  \n",
      " Session tracking  \n",
      " Page redirect  \n",
      " CRUD in Servlet (Create, Read, Update and Delete)  \n",
      " Servlet Application:  \n",
      "o Registration Example  \n",
      "o Uploading/downloading file  \n",
      "o Write Data to PDF  \n",
      "o Servlet sending mail  \n",
      " \n",
      "Module 3: JDBC and MVC  \n",
      " JDBC SQL Syntax and Environment  \n",
      " Driver types  \n",
      " JDBC Connection  \n",
      " JDBC Statements  \n",
      " JDBC Result set  \n",
      " Introduction to MVC  \n",
      " Creating sample web application – JSP, SERVLET, JDBC  \n",
      " \n",
      "Module 4: Spring Framework  \n",
      " Spring Framework Architecture  \n",
      " Dependency injection  \n",
      " IOC container  \n",
      "********************************************************************************\n",
      " Spring Modules and uses  \n",
      " Spring core - bean creation, life cycle of bean, scope of bean  \n",
      " Spring MVC  \n",
      " \n",
      "Module 5 : Hibernate Framework  \n",
      " Introduction to ORM  \n",
      " Cons in JDBC  \n",
      " Hibernate Architecture  \n",
      " Hibernate configuration, session, persistent class  \n",
      " Mapping files and mapping types  \n",
      " Hibernate query languages  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "21.  CSE  240       Data Science with R   3 Cr  \n",
      " \n",
      "CO1:  Understand the fundamental syntax of R through practice exercises, demonstrations, and \n",
      "writing R code  \n",
      "CO2:  Explore various operators, implement Control Structures and Functions for R Programs.  \n",
      "CO3:  Visualizing data using R with a different type of graphs and charts  \n",
      "CO4:  Analyzing and testing the data using statistical techniques like ANOVA, Time series, and \n",
      "Hypothesis test  \n",
      "CO5:  Ability to understand and apply to scale up machine learning techniques and associated \n",
      "computing techniques and technologies.  \n",
      " \n",
      "Module 1 R basics  \n",
      " Introduction to R: R console, Packages, Libraries, and Workspace  \n",
      " Data Types: Variables, and Strings, Modifyin g data types  \n",
      " Data structures in R: Vectors, Factors, Vector operations, Arrays, Matrices, Lists, and Data \n",
      "frames  \n",
      " Programming fundamentals: Conditions and loops, Functions in R, Objects and Classes, \n",
      "Debugging  \n",
      "Module 2 Data, functions and control structures in R \n",
      " Reading CSV, Excel Files, text files  \n",
      " Writing and saving data objects to file in R  \n",
      " Preparing Data in R: Data Cleaning, Tidy Data Concepts, Data imputation and Data \n",
      "conversion  \n",
      " String operations in R  \n",
      " Regular Expressions  \n",
      " Dates in R  \n",
      " Working with Built in functions, Working with User -defined functions  \n",
      " If statement,  If..else statement, Nested if..else  \n",
      " Switch structure  \n",
      " For loop, While loop, Repeat loop  \n",
      "Module 3 Data Visualization  \n",
      " Bar/pie/line chart/histogram/boxplot/scatter/density  \n",
      " Using Base Package, Lattice  Package, ggplot2 package  \n",
      " Combining Plots  \n",
      " Analysis with ScatterPlot, BoxPlot, Histograms, Pie Charts  \n",
      " Set.Seed Function , Preparing Data for Plotting  \n",
      " QPlot, ViolinPlot  \n",
      "Module 4 Statistical Analysis Using R  \n",
      " Data Summarization: Measures of Centre, Measures of Dispersion, Skewness and Kurtosis  \n",
      " Probability Distribution Using R  \n",
      " Hypothesis testing: t - Tests  \n",
      " Analysis of Variance: One -way ANOVA, Two -way ANOVA  \n",
      " Time series data and their graphical representation  \n",
      "********************************************************************************\n",
      "Module 5 Data Analytics Using R  \n",
      " Correlation Analysis  \n",
      " Simple Linear Regression  \n",
      " Multiple linear regression  \n",
      " Unsupervised Learning using k -means clustering  \n",
      " Supervised Learning - Classification  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      " \n",
      " \n",
      "22.  INT 300  INTERNSHIP  2 – YEAR 2  \n",
      " \n",
      "CO1  Use knowledge and skills acquired in science and engineering to explore and critically analyse \n",
      "a real -world problem; use creativity and innovation to identify and critically evaluate potential \n",
      "solutions to the problem; choose and implement a particular solution  \n",
      " ( Problem statement, Methodolo gy, Innovation, Project comprehension)  \n",
      "CO2  Apply conceptual knowledge for practical implementation of real time industry projects; \n",
      "Design, implement and evaluate a software system to meet desired needs within relevant \n",
      "constraints; evaluate and analyse the outcomes and impact of the system                 \n",
      " ( Design, Implementation & Broader impact)  \n",
      "CO3  Use of modern techniques and tools, communication tools, standard documentation and testing \n",
      "practices (Usage of modern tool/technology)  \n",
      "CO4  Conduct experiments to test software systems and interpret results to debug and improve \n",
      "system; Enhance the problem solving skills and achieve the proposed solution   \n",
      "(Real time Testing & Result/Outcome)  \n",
      "CO5  Work effectively toward a common objective in software development teams and contribute \n",
      "effectively; Apply verbal and written communication skills to explain technical problem \n",
      "solving techniques and solutions to global audience  \n",
      "(Each Students' contribution, Communication , Presentation  content)  \n",
      "CO6  Exhibit independent learning and professional development an d practice life -long learning \n",
      "(Learning from resources & Future scope)  \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "23. ENG 226 – English 2                                                              3 Cr  \n",
      "CO1  Enhanced vocabulary skills and proficiency in English language  \n",
      "CO2  Enhanced comprehension of native English speaking and ability to clear TOEFL Listening \n",
      "module  \n",
      "CO3  Enhanced professional English speaking skills and ability to clear TOEFL Speaking module  \n",
      "CO4  Enhanced reading comprehension of everyday English and ability to clear TOEFL Reading \n",
      "module  \n",
      "CO5  Enhanced professional English writing skills and ability to clear TOEFL Writing module  \n",
      " \n",
      "Unit 1 – LANGUAGE PROFICIENCY  \n",
      "Suffixes and Prefixes -Synonyms and Antonyms – Homonyms, Homophones, Homographs – \n",
      "Collocation -Connotation - Words  often Confused - One Word Substitution - Idioms and Phrasal Verbs - \n",
      "Exercises  \n",
      " \n",
      "Unit 2– LISTENING  \n",
      "Prosodic Features: Stress, Intonation, Pause, Pitch etc. – Advanced Listening Exercises: messages  \n",
      "and announcements, Songs, conversation with a native speaker et c. – TOEFL Listening: gist -content  \n",
      "and gist -purpose questions, detail questions, function questions, attitude questions, organization  \n",
      "questions, connecting content questions, inference questions.  \n",
      " \n",
      "Unit 3 – SPEAKING  \n",
      "Business Etiquette: Introduction and Greeting, Business Meetings, Oral Presentations, Telephonic  \n",
      "Interviews – TOEFL Speaking: Independent Task, Integrated Task: Read, Listen and Speak, Listen  \n",
      "and Speak  \n",
      " \n",
      "Unit 4 – READING  \n",
      "Types of Reading: Skimming, Scanning, Reading for Understanding, Critical  Reading – Newspaper  \n",
      "Reading – Note Making – Cloze tests – Advanced Reading Exercises – TOEFL Reading: factual  \n",
      "information and negative factual information, Inference and rhetorical purpose questions, vocabulary  \n",
      "questions, sentence simplification questions , insert text questions, prose summary and fill in a table  \n",
      "questions  \n",
      " \n",
      "Unit 5 – WRITING  \n",
      "Precise - Developing Outline - Email - Notice, Agenda, Minutes - Creative Writing: Newspaper Story  \n",
      "Writing, Movie Script Writing – TOEFL Writing: Integrated Task, Independen t Task  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "24. CSE 230 Mobile Programming                                                  3 Cr  \n",
      " \n",
      "CO1  Demonstrate the Understanding of fundamentals of Android Programming.  \n",
      "CO2  Build their ability to apply various components in mobile application development.  \n",
      "CO3  Ability to develop user interactions with lists and views  \n",
      "CO4  Discover the life cycles of Activities, Applications, intents, and fragments.  \n",
      "CO5  Design, develop case studies and publish them in Google play  \n",
      " \n",
      "Module 1: Fundamentals of Mobile Programming  \n",
      " Introduction to Android Application Development  \n",
      " Android Ecosystem  \n",
      " Android Studio  \n",
      " Emulators  \n",
      " Gradle Build System  \n",
      " Manifest File  \n",
      " Introduction to Resources (Strings, Drawables etc)  \n",
      " The R.java file  \n",
      " \n",
      "Module 2: Android Components  \n",
      " Layouts  \n",
      " Text views  \n",
      " Buttons  \n",
      " Edit texts  \n",
      " Image View  \n",
      " Checkbox  \n",
      " Radio Buttons  \n",
      " Toggle Buttons  \n",
      " Spinner  \n",
      " \n",
      "Module 3: User Interactions  \n",
      " Toast Messages  \n",
      " Snack bar Messages  \n",
      " Dialog Messages  \n",
      " Constraint Layout  \n",
      " List View  \n",
      " Recycler View  \n",
      " Grid View  \n",
      " Scroll View  \n",
      " WebView  \n",
      " \n",
      "Module 4: Lifecycles &  Shared Preferences  \n",
      " Application Lifecycle  \n",
      " Activity & Lifecycle  \n",
      " Fragment & Lifecycle  \n",
      " Services  \n",
      "********************************************************************************\n",
      " Receivers  \n",
      " Intents  \n",
      " Shared Preferences Class  \n",
      " Saving Data Local Memory  \n",
      " Calling Back Data  \n",
      " \n",
      "Module 5: Case studies & Publishing App on Google Play  \n",
      " Case studies  \n",
      " APK Release Version  \n",
      " APK files and its types  \n",
      " Google Play Developer Account  \n",
      " Release Your App  \n",
      " \n",
      "References:  \n",
      "1. https://www.udemy.com/course/full -stack -android -development -and-mobile -app-marketing/  \n",
      "2. https://www.udemy.com/course/mobile -app-development -with-android -2015/  \n",
      "Reference Books:  \n",
      "1. Android: A Programming Guide by J.F. DiMarzio.  \n",
      "2. Hello, Android: Introducing Google's Mobile Development Platform by Ed Burnett.  \n",
      "3. Programmi ng android by Zigurd Mednieks.  \n",
      "4. Android User Interface Design: Turning Ideas and Sketches into Beautifully Designed  \n",
      "Apps byIan G. Clifton.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "25. CSE 260 Data Visualization Tools and Techniques                                        3 Cr \n",
      "CO1  Understand the importance of data visualization and demonstrate the ability to choose \n",
      "the right tool and technique for the given business problem  \n",
      "CO2  Demonstrate the ability to use Tableau software to analyze and visualize time series \n",
      "data \n",
      "CO3  Design and develop tableau applications using maps and understand the basics of \n",
      "colors, views, and other critical visualization -based issues  \n",
      "CO4  Demonstrate the ability to use Tableau software to plan, design, layout and implement \n",
      "dashboards and stories.  \n",
      "CO5  Design and implement advanced dashboards using Tableau  \n",
      " \n",
      "Module 1 Data Visualization – Introduction to Tableau  \n",
      "Overview of different types of data visualization tools and their features  \n",
      "Choosing the right visual and the right tool  \n",
      "Factors that influence the choice of right tool  \n",
      "Types of data visualization techniques and choosing the right technique  \n",
      "Installation and Use of Tableau  \n",
      " \n",
      "Module 2 Tableau  \n",
      "Understanding the layout and the concept of cards / shelves  \n",
      "Creating, formatting and displaying bas ic charts  \n",
      "Creating multiple types of charts (including maps, heat maps and tree maps)  \n",
      "Using trend lines to analyze  and display time series data  \n",
      "Working with data hierarchies  \n",
      "Using filters, sets, groups and hierarchies to enhance views  \n",
      " \n",
      "Module 3 Tableau Application  \n",
      "Visual Analysis  \n",
      "Maps and Locations  \n",
      "Color Vision Deficiencies  \n",
      "Calculations  \n",
      " \n",
      "Module 4 Data Visualization Presentation: Dashboards and Storytelling  \n",
      "Creative data visualization  \n",
      "Selection and presentation of key performance indicators (KPIs)  \n",
      "Element s of graphic design for visualization and project web site design  \n",
      "Creating interactive dashboards using techniques like filtering and highlighting  \n",
      "Dashboard planning, design, layout and implementation  \n",
      "Incorporating multimedia objects  \n",
      "Methods for animating data visualizations to illustrate change  \n",
      "Developing effective and engaging data stories  \n",
      "Decision making using data visualization  \n",
      " \n",
      "Module 5  \n",
      "Advanced Dashboards  \n",
      "Dashboard Applications  \n",
      "********************************************************************************\n",
      "Data Visualization Certification  \n",
      " \n",
      "References  \n",
      "https://www.udemy.com/course/ complete -tableau -bootcamp -dashboards/  \n",
      "https://www.udemy.com/course/tableau -2018 -tableau -10-qualified -associate -certification/  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "********************************************************************************\n",
      "26. MTH 221 Differential Equations                                                            3 Cr \n",
      "CO1  Understanding types of differential equations and solving first order differential equations  \n",
      "CO2  Solving second order differential equations and dealing with applications  \n",
      "CO3  Solving PDE and using PDE as a tool to solve real -world applications  \n",
      "CO4  Learning various properties of Laplace and Inverse Laplace transforms to solve DE  \n",
      "CO5  Enabling the students to learn various properties of Fourier tra nsform  \n",
      " \n",
      "Module 1 - First order Differential Equations  \n",
      " Introduction to differential equations  \n",
      " Order and degree  \n",
      " Homogeneous equations and Non -homogeneous equations  \n",
      " Solution by separating variables  \n",
      " Exact equations  \n",
      " Solving first order linear differential equations  \n",
      " Bernoulli’s Equation  \n",
      " \n",
      "Module 2 – Second order Differential Equations  \n",
      " Solving second order differential equations with constant coefficients  \n",
      " Homogenous solutions  \n",
      " Non-homogeneous solutions  \n",
      " Superposition Principle  \n",
      " Initial value problems(IVP)  \n",
      " Wronskian  \n",
      " \n",
      "Module 3 – Partial Differential Equations  \n",
      " Introduction  \n",
      " Equations solvable by direct integration  \n",
      " Linear equations of the first order  \n",
      " Lagrange’s linear equation  \n",
      " Solution of equation by method of separation of variable  \n",
      " \n",
      "Module 4 - Laplace Transforms  \n",
      " Introduction  \n",
      " Elementary Properties  \n",
      " Derivative Property of the Laplace Transform  \n",
      " Inverse Laplace Transform  \n",
      " Solving IVP using Laplace transform  \n",
      " \n",
      "Module 5 -Fourier Transforms  \n",
      " Statement of Fourier integral theorem  \n",
      " Fourier transform pair  \n",
      " Fourier sine  and cosine transforms  \n",
      " Properties  \n",
      " Transforms of simple functions  \n",
      "********************************************************************************\n",
      " Convolution theorem  \n",
      " Parseval’s identity.  \n",
      "********************************************************************************\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "********************************************************************************\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [ 21] and conditional\n",
      "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n",
      "********************************************************************************\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "********************************************************************************\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4\n",
      "********************************************************************************\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n",
      "5\n",
      "********************************************************************************\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6\n",
      "********************************************************************************\n",
      "length nis smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\n",
      "orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate =d−0.5\n",
      "model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup _steps = 4000 .\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7\n",
      "********************************************************************************\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop= 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8\n",
      "********************************************************************************\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dvPdrop ϵlstrain PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B)16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "********************************************************************************\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor .\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "References\n",
      "[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450 , 2016.\n",
      "[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "10\n",
      "********************************************************************************\n",
      "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR , abs/1406.1078, 2014.\n",
      "[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357 , 2016.\n",
      "[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "network grammars. In Proc. of NAACL , 2016.\n",
      "[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 770–778, 2016.\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\n",
      "9(8):1735–1780, 1997.\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722 , 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130 , 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "11\n",
      "********************************************************************************\n",
      "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\n",
      "2006.\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859 , 2016.\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research , 15(1):1929–1958, 2014.\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems , 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages 434–443. ACL, August 2013.\n",
      "12\n",
      "********************************************************************************\n",
      "Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "********************************************************************************\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "********************************************************************************\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.text)\n",
    "    print(\"*\"*80)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:51:12.146888Z",
     "start_time": "2024-09-21T12:51:12.128435Z"
    }
   },
   "id": "3cee5cd1638988c6",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(documents))\n",
    "documents[3].text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:35:52.649705Z",
     "start_time": "2024-09-21T12:35:52.643625Z"
    }
   },
   "id": "1635b310957a789",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:19:08.206384Z",
     "start_time": "2024-09-21T12:19:07.205190Z"
    }
   },
   "id": "835daeb36f94517f",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:19:49.224089Z",
     "start_time": "2024-09-21T12:19:49.174505Z"
    }
   },
   "id": "f7076a259f6c3d92",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base model was trained for a total of 100,000 steps or 12 hours, while the big model was trained for 300,000 steps (3.5 days).\n"
     ]
    }
   ],
   "source": [
    "query = '''What is the base train steps and also big'''\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:43:45.938089Z",
     "start_time": "2024-09-21T12:43:44.566666Z"
    }
   },
   "id": "c1d31d84f5f986b2",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T12:50:34.711246Z",
     "start_time": "2024-09-21T12:50:34.705205Z"
    }
   },
   "id": "b2baf11ffdfa25e7",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "97f0a4cd923fe8fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
